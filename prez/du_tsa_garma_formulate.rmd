## Formulating ARMA process

In this section, we start to define more general autoregressive and moving average process. First we define a moving average of order q:

$$\begin{equation}
	y_t=\varepsilon_t+\alpha_1 \varepsilon_{t-1}+\alpha_2 \varepsilon_{t-2}+\dotsb+\alpha_q \varepsilon_{t-q}
	\label{eq:maq}
\end{equation}$$

Here, $y_t=Y_t-\mu$ and $\varepsilon_t$ is a white noise process. It also means that the demeaned series $y_t$ is a weighted combination (can we say weighted average also?) of $q+1$ white noise terms. Here is $q+1$ terms since the $q$ starts from the second term. 
On the other hand, an autoregressive process of order $p$, which is denoted as $AR(p)$, can be expressed as:

$$\begin{equation}
    y_t=\theta_1 y_{t-1}+\theta_2 y_{t-2}+\theta_3 y_{t-3}+\dotsb+\theta_p y_{t-p}+\varepsilon_t
    \label{eq:arq}
\end{equation}$$

---

Obviously it is very much possible to combine the $MA(q)$ and $AR(p)$ and come up with a $ARMA(p,q)$ specification of the following form:

$$\begin{equation}
    y_t=\theta_1 y_{t-1}+\theta_2 y_{t-2}+\dotsb+\theta_p y_{t-p}+\varepsilon_t+\alpha \varepsilon_{t-1}+\dotsb+\alpha_q \varepsilon_{t-q}
\end{equation}$$

Now, the next million dollar question is  when to choose an $AR$, $MA$ or $ARMA$  Verbeek remains quite vague about this at this point. He says, $AR$ and $MA$ basically same time of series. It's just a matter of parsimony. What's the meaning of this? We have previously seen that $AR(1)$ can be expressed as infinte lag order of $MA$ series. So, what does parsimony means here? Does this mean that if we weant a smaller model then we choose $AR(1)$ over infinite order $MA$? Verbeek says it will be clear later. At this point, it is difficult to know. We will postpone the discussion for now, come back to it later. Now we will talk briefly on $\textbf{Lag Operator}$.

---

## Lag operator

In the notation of time series model, it is often convenient to use lag operator($L$) or backshift operator($B$) which some author use. But we will stick to $L$ here. Let's see a use:

$$\begin{equation}
    Ly_t=y_{t-1}\\
    L^2y_t=L.Ly_t=L.y_{t-1}=y_{t-2}\\
    \vdots \\
    L^q y_t=y_{t-q}
    \label{}
\end{equation}$$

There are other relationships involving $L$, such as $L^0 \equiv 1$ and $L^{-1}=y_{t+1}$. Use of this notions makes life much simpler in specifying a long time series specification such as an $ARMA$ model quite concisely. For example, let's start with an $AR(1)$ model:

$$\begin{equation}
    \begin{split}
        y_t & =\theta y_{t-1}+\varepsilon_t \\
            & =\theta L.y_t+\varepsilon_t \\
            y_t-\theta Ly_t & = \varepsilon_t \\
            (1-\theta L)y_t &= \varepsilon_t
    \end{split}
    \label{eq:eqL}
\end{equation}$$

---

We can refer to \ref{eq:eqL} as follows: here, $y_t$ and it's one period lag $y_{t-1}$ , on the right hand side, is a combination with weights of 1 and $-\theta(L)$ and equals a white noise process ($\varepsilon_t$). In general, we can write $AR(p)$ as follows:

$$\begin{equation}
    \theta(L)y_t=\varepsilon_t
    \label{eq:eqargen}
\end{equation}$$


Here $\theta(L)$ is a polynomial\footnote{What's the definition of polynomila?} of order $p$ . I think $\theta(L)$ is some kind of function of $L$. We could write it like $f(L)$. Writing it like $\theta(L)$ creates some confusion because there is also $\theta$ in the function. Now, $\theta(L)$ can be expressed as: 

$$\begin{equation}
    \theta(L)=1-\theta L -\theta_2 L^2-\dotsb-\theta_p L^p
    \label{eqlagpol}
\end{equation}$$

We can interpret lag polynomial as a filter. When we apply it to a time series, it produces a new series. 

---

## Characteristics of Lag Polynomial

Suppose we apply a lag polynomial to a time series. Then we apply another lag polynomial on the top of that series. This is equivalent to applying product of two lag polynomial on the original series. We can also define the inverse of a filter, which is the inverse of polynomial. The inverse of $\theta(L)$ is $\theta^{-1}(L)$ and we can write $\theta(L) \theta^{-1}(L)=1$ . Now the following statement I really don't understand. 

If $\theta(L)$ is a finite order polynomial, then $\theta{-1}L$ is a infinite order polynomial in $L$. It also goes onto claim that

$$\begin{equation}
    (1-\theta L)^{-1}=\sum_{j=0}^{\infty}\theta^j L^j
    \label{eq:lagpol2}
\end{equation}$$

provided that $|\theta|<1$. Now the question is how to prove that claim? 
Now let's start with simply $\sum_{j=0}^\infty \theta^j$. 

$$\begin{align}
    \sum_{j=0}^\infty & = \theta_0+\theta_1+\dotsb \\
    & =\frac{1-\theta^\infty}{1-\theta} \quad \text{provided that} \quad |\theta|<1 \\
    &=\frac{1}{1-\theta}
    \label{eq:eqthet}
\end{align}$$

---

Now let's see this version:
$$\begin{align}
    \sum_{j=0}^\infty \theta^j L^j & = \theta^0 L^0+ \theta^1 L^1 + \theta^2 L^2+ \dotsb \\
    & = 1+\theta L + \theta^2 L^2+\dotsb \\
    & = \frac{1-(\theta L)^\infty}{1-\theta L} \\
    & = \frac{1-\theta^\infty L^\infty}{1-\theta L} \\
    & = \frac{1}{1-\theta L} \quad \text{by} \quad |\theta|<1\\
    & = (1-\theta L)^{-1} \\
    \text{So we find that,}
    \sum_{j=0}^\infty \theta^j L^j & = \frac{1}{1-\theta L}=(1-\theta L)^{-1}
    \label{eqthl}
\end{align}$$

Now, we have seen in \ref{eq:eqL} that $(1-\theta L)y_t=\varepsilon_t$. 

---

From this it follows that:

$$\begin{align}
    (1-\theta L)^{-1}(1-\theta L) y_t & = (1-\theta L)^{-1} \varepsilon_t \\
    y_t & = (1-\theta L)^{-1} \varepsilon_t \\
    \text{or,} \qquad y_t & = \sum_{j=0}^\infty \theta^j L^j \varepsilon_t \\
    \text{From, the previous definition of Lag operator($L$)}
    L\varepsilon_t & = \varepsilon_{t-1} \\
    L^2 \varepsilon_t & =\varepsilon_{t-2} \\
    \vdots \\
    L^j \varepsilon_t &= \varepsilon_{t-j} \\
    \text{Therefore, we can write}
    y_t & = \sum_{j=0}^\infty \theta^j \varepsilon_{t-j}
    \label{}
\end{align}$$

---

## From $MA(1)$ to $AR(\infty)$

It corresponds to same derivation where we have shown that $AR(1)$ corresponds to $MA(\infty)$ series. We have said previously that $MA(1)$ can also be transformed into a $AR$ series but could not show it quite mathematically. Armed with lag operator $(L)$, we can try to show it here. 

$$\begin{align}
    \text{We know that in}\, MA(1) \\
    y_t   &= \varepsilon_t+\alpha \varepsilon_{t-1} \\
          &= \varepsilon_t+\alpha L \varepsilon_t \\
          &= (1+\alpha L)\varepsilon_t
(1+\alpha L)^{-1}y_t &= \varepsilon_t
\end{align}$$

---

Now, let's see how we can define $(1+\alpha L)^{-1}$.

$$\begin{align}
    (1+\alpha L)^{-1} &= \frac{1}{(1+\alpha L)} \\
    & = \frac{1-(-\alpha L)^{\infty}}{1-(-\alpha L)} \\
    & = -\alpha L+(-\alpha L)^2+ (-\alpha L)^3+\dotsb \\
    & = \sum_{j=0}^\infty (-\alpha L)^j \\
    \text{Therfore, we write that } \\
    (1+\alpha L)^{-1} & = \sum_{j=0}^\infty (-\alpha L)^j \\
    \text{Basically, we see that} \\
    \varepsilon_t &=\sum_{j=0}^\infty (-\alpha)^j (L)^j y_t\\
    &=\sum_{j=0}^\infty (-\alpha)^j y_{t-j}
\end{align}$$

---

Now, let's figure out $\varepsilon_{t-1}$. This is important since we write  $MA(1) = \alpha \varepsilon_{t-1}+\varepsilon_t$. What we will do is put the value of $\varepsilon_{t-1}$ and keep $\varepsilon_t$ as it is

$$\begin{align}
    y_{t-1} & =\varepsilon_{t-1}+\alpha \varepsilon_{t-2} \\
    & =\varepsilon_{t-1} + \alpha L \varepsilon_{t-1} \\
    & =\varepsilon_{t-1}(1+\alpha L) \\
\text{or,}\qquad \varepsilon_{t-1} & =(1+\alpha L)^{-1} y_{t-1} \\
& = \sum_{j=0}^\infty (-\alpha L)^j y_{t-1} \\
& = \sum_{j=0}^\infty (-\alpha)^j L^j y_{t-1} \\ 
& = \sum_{j=0}^\infty (-\alpha)^j  y_{t-j-1} \\                    
\text{So, we can write}
\varepsilon_{t-1} &= \sum_{j=0}^\infty (-\alpha)^j  y_{t-j-1} \\                    
\text{Now getting back to the}\, MA(1) \\
y_t & =\alpha \varepsilon_{t-1}+\varepsilon_t \\
    & =\alpha \sum_{j=0}^\infty (-\alpha)^j  y_{t-j-1}+ \varepsilon_t \\ 
\end{align}$$

---

We here require that \textit{lag polynomial} in the $MA(1)$ which is $(1+\alpha L)$ is invertible. It can be shown that it is invertible only if $|\alpha|<1$. $AR$ representation is very convenient when we think that current behavior is determined by past behavior. $MA$ representation is quite useful for determining variance and covariance of the process. It is not quite clear how so but it might be clear in the subsequent analysis. 


---

## Parsimonious representation of ARMA

We have seen previously the following $ARMA$ representation:

$$\begin{gather}
    y_t  =\theta_1 y_{t-1}+\theta_2 y_{t-2}+\dotsb+\theta_p+\varepsilon_t+\alpha \varepsilon_{t-1}+\dotsb+\alpha_q \varepsilon_{t-q} \\
    \text{or,} \qquad y_t-\theta_1 y_{t-1}-\dotsb-\theta_p y_{t-p}  =(\alpha(L))\varepsilon \\
\text{or,} \qquad \theta (L) y_t  =\alpha (L) \varepsilon_t \\
\text{Now, if we think lag polynomical in}\, AR(1)\, \text{is invertible, then}\\ 
y_t  =\theta^{-1}(L) \alpha (L) \varepsilon_t \\
\text{On the other hand, if lag polynomial in}\,  MA(1)\, \text{is invertible, then we can write} \\
\alpha^{-1}(L) \theta (L) y_t  =\varepsilon_t
    \label{eq:eqpararma}
\end{gather}$$




