<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>TIME SERIES LECTURES</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs-2.3/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/hygge.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/ninjutsu.css" rel="stylesheet" />
    <link rel="stylesheet" href="myprez.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# TIME SERIES LECTURES

---










## Stationarity 

--

A stochastic process is said to be stationary if it's properties are unaffected by a change of time origin.

--

Time origin might mean the particular time period which represents the time range. 

--

It might be the starting time period or the center value of time. 

--

In other words, joint probability distribution of any set of times does not change shape or affected by any means if there is a shift in the time axis. 

--

Why we talk about joint probability distribution here?  

--

This is because every value in every time period is a particular value of a random variable. 

--

For example, `\(Y_t\)` is random variable, same as `\(Y_{t-1}\)` is a random variable.

--

When a time-series consists of these two variables, the distribution of this time series is a joint distribution.


---

## Strict stationarity

--

A time series model `\({x_t}\)` is strictly stationary if the joint distribution of `\(x_1,\ldots,x_{t_n}\)` is the same as the joint distribution of `\({x_{1+m},\ldots,x_{t+m}}\)` for all `\(t_1,\ldots,t_n\)` and m, so that the distribution is unchanged after an arbitrary time shift.

--

Strict stationarity implies that the mean and variance are constant in time

--

Autocovariance `\(Cov(x_t, x_s)\)` only depends on lag `\(k=|t-s|\)` and can be written `\(\gamma(k)\)` 

---

## Weak stationarity

--

If a series is not not strictly stationary but the mean and variance are constant in time and autocovariance only depends on the lag, then the series is called second-order stationary.

--

Usually stationarity is a very strict condition which requires that the whole distribution must be unchanged at any time period. 

--

It means that all the moments in any order should be equal. 

--

But this is usually impractical to find. 

--

As a result, we impose a softer version of this \textbf{strict stationarity} which we call \textbf{weak stationarity}. 

--

Weak stationarity is a situation when we have just mean, variance are equal. 

---

## Covariance stationary


We also have covariance which does not change we time difference between variables are the same. This means:


`$$\begin{equation}
    Cov(Y_1,Y_5)=Cov(Y_{17},Y_{21})
\end{equation}$$`


 This is also called covariance stationary. 

--

 We can formally express the covariance stationary in the following way:

`$$\begin{align}
	E(Y_t)&amp;=\mu &lt; \infty \\ 
	Var(Y_t)&amp;=\sigma^2 \\
	Cov(Y_t,Y_{t-k})&amp;=E[(Y_t-E(Y_{t-1}))(Y_{t-k}-E(Y_{t-k}))] \\ 
	&amp;=E[(Y_t-\mu)(Y_{t-k}-\mu)]\\
	&amp;=\gamma_k \label{sco} \quad k=1,2,3,\dotsc
\end{align}$$`

In the above, we find that,  weakly stationary process is characterized by constant finite mean and variance. 


It also imply that covariance between any two time periods does not depend on the time period they are in, rather it depends on the time lag between these two variables. 

???

If the white noise is Gaussian, the stochastic process is commpletely defined by the mean and covariance structure, in the same way as any normal distribution is defined by its mean and variance-covariance matrix.

First step in any should be to check whether there is any evidence of a trend or seasonal effects

If there is we will have to remove them

It is often reasonable to treat the time series of residuals as a realisation of a stationary error series.

Therefore, these models in this topic are often fitted to residual series arising from regression analyses

---

## Autocorrelation function (ACF)

--

**Autocovariance**

--

Since here we are trying to find covariance between two variables where one is the lag version of the same variable, we call these `\(\textbf{autocovariances}\)`. 

Here we define the `\(\textbf{k-th order autocovariance}\)` as:

`$$\begin{equation}
    \gamma_k=Cov(Y_t,Y_{t-k})=Cov(Y_{t-k},Y_t)
\end{equation}$$`

When `\(k=0\)`, `\(\gamma_k\)` reduces to:

`$$\begin{equation}
    \gamma_0=Cov(Y_t,Y_t)=Var(Y_t)
    \label{gam0}
\end{equation}$$`

---

## Autocorrelation		

--

One problem with covariance its value is not independent of the units in which the variables are measured. 

--

For example, `\(Cov(Y_1,Y_5)\)`  and `\(Cov(Y_12, Y_16)\)` might not equal just because units are different. 

--

For example, let's say `\(Y_t\)` is measuring the CPI. In early time periods, CPI usually had lower values but suddenly it got a shift in later time periods. 

--

It's better to use correlation measure which is unit free, obviously we will use `\(\textbf{autocorrelation}\)` which standardizes to compare across different series. 

--

Here we will derive `\(\rho_k\)` which is the k-th order autocorreation:

`$$\begin{gather}
    \rho_k &amp; =\frac{Cov(Y_t,Y_{t-k})}{\sqrt{Var(Y_t)}\sqrt{Var(Y_{t-1})}} \\
    &amp; =\frac{\gamma_k}{\gamma_0} \quad (\text{by}\quad \eqref{gam0})
\end{gather}$$`

--

Obviously, here `\(\rho_0=\frac{Cov(Y_t,Y_t)}{\gamma_0}=\frac{Var(Y_t)}{\gamma_0}=\frac{\gamma_0}{\gamma_0}=1\)` 

And also as usual, `\(-1\le \rho_k \le 1\)` which is standard for autocorrelation measures.

---



class: inverse,center,middle

# Autogregressive models

---

## Specification

We will start with the simplest form of time-series model which is called first-order autoregressive models or AR(1).

--

`$$\begin{equation}
    Y_t=\alpha+\theta Y_{t-1}+\varepsilon_t
\end{equation}$$`

--

A simple way to model dependence between observations in different time periods would be that `\(Y_t\)` depends linearly on the observation from the previous time period `\(Y_{t-1}\)`.

--

* Here `\(\varepsilon_t\)` means serially uncorrelated innovation with mean of zero and a constant variance. 
--

* The process in the above equation is called a first order `\(\textbf{autoregressive process}\)` or `\(AR(1)\)` process. 

--

* This process tells us that the value of Y at time `\(t\)` depends on a constant term plus `\(\theta\)` times plus an unpredictable component `\(\varepsilon\)`. 

--

* Here we shall assume `\(|\theta|&lt;1\)`.  

--

* The process underlying `\(\varepsilon_t\)`  called `\(\textbf{white noise process}\)`. 

--

* Here `\(\varepsilon\)` will be always homoskedastic and will not show any autocorrelation. 

---

## Expected Value of AR(1)

The expected value of `\(Y_t\)` can be solved from 

`$$\begin{equation*} E(Y_t)=\delta+\theta E(Y_{t-1}) \end{equation*}$$`

--

which, assuming that `\(E(Y_t)\)` does not depend upon `\(t\)`, allows us to write 

--

`$$E(Y_t)=E(Y_{t-1})=\mu \label{}$$`

--

Using the above assumption, 

--

`$$\begin{equation}
    \mu=\delta+\theta\mu \\
    \mu - \theta \mu = \delta \\
    \mu(1-\theta)= \delta \\
    \mu =\frac{\delta}{1-\theta}
\end{equation}$$`

--

Remember, this is only true if `\(Y_t\)` is stationary, that means it's statistical property does not depend on a particular time period, in other words, the mean is constant. We will also write `\(y_t=Y_t-\mu\)`. Now it follows that 

---

Continuing on,

--

`$$\begin{gather*}
    y_t+\mu=\delta+\theta (y_{t-1}+\mu)+\varepsilon \\ 
    \text{In the above we have seen that} \\ 
    \mu\equiv E(Y_t)=\frac{\delta}{(1-\theta)} \\ 
    \text{which means} \\ 
    \delta=\mu (1-\theta) \\ 
    \text{Now putting this value in the above equation} \\ 
    y_t+\mu=\mu (1-\theta)+\theta y_{t-1}+\theta \mu+\varepsilon \\ y_t+\cancel{\mu}=\cancel{\mu}-\mu \theta+\theta y_{t-1}+\theta \mu+\varepsilon \\ 
    y_t=\cancel{\mu \theta}+\theta y_{t-1}+\cancel{\theta \mu}+\varepsilon \\ 
    y_t=\theta y_{t-1}+\varepsilon 
\end{gather*}$$`

???

Defining time series models in terms of `\(y_t\)` rather than `\(Y_t\)` is often notationally convenient. So `\(y_t\)` has been introduced so that depiction of time models remain concise

---

Taking expectation, we get :
`$$E(y_t)=\theta E(y_{t-1})+ E(\varepsilon)$$` 

--

Since `\(E(\varepsilon)=0\)` we can write 
`$$E(y_t)=\theta E(y_{t-1})$$` 

--

Remember, previously we said that `\(Y_t\)` is stationary and as a result `\(E(Y_t)=E(Y_{t-1})\)`

--

Then we can write,

--

`$$\begin{gather*}
    E(y_t+\mu) =E(y_{t-1}+\mu) \\
    E(y_t)+E(\mu)=E(y_{t-1})+E(\mu) \\
 \text{since}\,\mu\, \text{is a constant}\,E(\mu)=\mu  \\
    E(y_t)+\cancel{\mu}=E(y_{t-1})+\cancel{\mu} \\
    E(y_t)=E(y_{t-1}) \\
    \text{Now from the above, we can write} \\
    E(y_t)-\theta E(y_t)=0 \quad \text{since} \quad E(y_t)=E(y_{t-1}) \\
    E(y_t)=0
\end{gather*}$$`

Exaplanation: 

`$$E(y_t) (1-\delta) = 0$$`

If   `\(\delta\neq 1\)` then `\(1-\delta\neq 0\)`
If `\(1-\delta\neq 0\)`   then `\(E(y_t) =0\)`  


---

* Above results show that `\(y_t\)` has a zero mean. But to have a non-zero mean we can add an intercept. 

--

* We also here note that `\(V(y_t)=V(y_t)\)`. 

--

* The process described in above equation imposes certain restrictions on the stochastic process that generates `\(Y_t\)`. 

--

* Usually when we have a group of variables, we usually describe their joint distribution by  covariances. 

--

* Since, here are lagged version of other variables, we call it `\(\texttt{autocovariances}\)` .  

--

* This is the so called `\(\texttt{stationarity}\)` condition. 

--

* Basically we require stationarity to derive dynamic properties of `\(Y_t\)`.

--

* Without deriving dynamic properties of `\(Y_t\)`, we can not forecast its values. 

---

## Variance of AR(1)

Previously we derived the constant mean of the distribution by imposing that mean does not depend on `\(t\)`. Now we will do the same thing with variances. 

--

First let's derive the variance:


--

`$$\begin{equation*}
	\begin{split}
		Var(Y_t)&amp;=Var(\delta+\theta Y_{t-1}+\varepsilon_t)\\
		 &amp;=Var(\delta)+Var(\theta Y_{t-1}+\varepsilon_t) \\
		&amp;=Var(\theta Y_{t-1}+\varepsilon_t) \quad \because \quad Var(\delta)=0\\
		&amp;=\theta^2 Var(Y_{t-1})+Var(\varepsilon_t) 
	\end{split}
\end{equation*}$$`

--

Therefore, we can write

    
Here we assume that `\(Cov(Y_{t-1} \varepsilon_t)=0\)`. Now, this is not too unrealistic in the sense that error at current period might not be correlated with the endogenous variable in the past.Now it's time to impose one of the stationarity condition, namely, variance of the time series process does not depend on time:

`$$\begin{equation}\label{eqst}
		Var(Y_t)=Var(Y_{t-1})
	\end{equation}$$`

---

Now using above equations,

`$$\begin{gather}
    Var(Y_t) =\theta^2 Var(Y_t)+ \sigma^2 \quad \text{by} \quad Var(\varepsilon_t)=\sigma^2\\
		\text{or,} \quad Var(Y_t)(1-\theta^2) =\sigma^2\\
		\text{or,} \quad Var(Y_t) =\frac{\sigma}{1-\theta^2} \\
		\text{or,}   \gamma_0 = \frac{\sigma}{1-\theta^2}
\end{gather}$$`


We see from the above equation  that `\(Var(Y_t)\)` is indeed constant. We also see from above that we can only impose `\(Var(Y_t)=Var(Y_{t+1}\)` if `\(|\theta|&lt;1\)` . This is the assumption we madel previously.This is actually the essence of `\(\texttt{Dickey-Fuller}\)` test which tests whether this coefficient is less than one or not. 

---

## Covariances of AR(1)	

Now let's find the `\(Covariance\)` between `\(Y_t\)` and `\(Y_{t-1}\)`

`$$\begin{equation} 
    \begin{split} 
	Cov(Y_t,Y_{t-1}) &amp; =E(Y_t-E(Y_t))E(Y_{t-1}-E(Y_{t-1})) \\ 
		     &amp; =E((Y_t-\mu)(Y_{t-1}-\mu))  \quad \because \quad E(Y_t)=E(Y_{t-1})=\mu \\
		     &amp; =E(y_t y_{t-1}) \quad \text{by the definition of}\quad y_t\\
		     &amp; =E((\theta y_{t-1}+\varepsilon_t)(y_{t-1})) \because \, y_t=\theta y_{t-1}+\varepsilon_t \\
                     &amp; =\theta (E(y_{t-1})^2+E(\varepsilon_t y_{t-1}) \\
		     &amp; =\theta \, E(Y_{t-1}-\mu)^2+E(\varepsilon_t y_{t-1})\\
		     &amp; =\theta \, Var(Y_t)+E((\varepsilon_t-E(\varepsilon_t)) (Y_{t-1}-\mu))\\
		     &amp; =\theta \, Var(Y_t)+Cov(\varepsilon_t,Y_t)\\
		     &amp; =\theta \, Var(Y_t)\quad \because \quad Cov(\varepsilon_t,Y_{t-1})=0
		\label{eqcov}
    \end{split} 
\end{equation}$$`

So, we have established that 

`$$\begin{equation}
	Cov(Y_t,Y_{t-1})=\gamma_1=\theta \frac{\sigma^2}{1-\theta^2}
\end{equation}$$`

---

Now let's take some higher order lags. Let's see what would be the covariance between `\(Y_t\)` and `\(Y_{t-2}\)`, that is autocovariance of lag order 2.


`$$\begin{equation}
		\begin{split}
		Cov(Y_t,Y_{t-2}) &amp; =E(Y_t-E(Y_t))E(Y_{t-2}-E(Y_{t-2}))\\
		&amp; =E((Y_t-\mu)(Y_{t-2}-\mu))\quad \because \quad E(Y_t)=E(Y_{t-2})=\mu\\
		&amp; =E(y_t\, y_{t-2})\quad \text{by the definition of} Y_t\\
\text{Before continuing we have to figure out} \, y_t\\
		y_t &amp; =\theta y_{t-1}+\varepsilon_t\\
		&amp; =\theta (\theta y_{t-2}+\varepsilon_{t-1})+\varepsilon_t\\
		&amp; =\theta^2 y_{t-2}+\theta \varepsilon_{t-1}+\varepsilon_t\\
\text{Now,putting this value back in equation}
		&amp; =E((\theta^2 y_{t-2}+\theta \varepsilon_{t-1}+\varepsilon_t)y_{t-2})\\
		&amp; =\theta^2 (E(y_{t-2})^2+E(\varepsilon_{t-1} y_{t-2})+E(\varepsilon_t y_{t-2}))\\
		&amp; =\theta^2 \, E(Y_{t-2}-\mu)^2+Cov(\varepsilon_{t-1},y_{t-2})+Cov(\varepsilon_t,y_{t-2})\\
		&amp; =\theta^2 \, Var(Y_t)\quad \because \quad Cov(\varepsilon_t,Y_{t-k})=0\\
		&amp; =\theta^2 \, \frac{\sigma^2}{1-\theta^2}
		\end{split}
\end{equation}$$`

---

`$$\begin{equation}
    \text{Therefore we see that,} \\
     Cov(Y_t,Y_{t-1}) = \gamma_1=\theta \frac{\sigma^2}{1-\theta^2} = \theta \gamma_0\\
     Cov(Y_t,Y_{t-2}) =\gamma_2=\theta^2 \frac{\sigma^2}{1-\theta^2} = \theta^2 \gamma_0\\
    \vdots \\
     Cov(Y_t,Y_{t-k}) =\gamma_k=\theta^k \frac{\sigma^2}{1-\theta^2} = \theta^k \gamma_0
\end{equation}$$`

We have the following observation from the above Covariance formula


* As long as `\(\theta\)` is non-zero, any two observation on `\(Y\)` has non-zero correlation. This might imply that as long as `\(Y\)` in period `\(t\)` is correlated with it's immediate past value, that correlation carries through higher degree of past value.
* Since the value of `\(|\theta|&lt;1\)`, this correlation diminishes in higher degree of lag-order. This is quite intuitive as itis natural to have weakerza correlation among values whic are far apart. 
* Covariance between any two time periods does not depend on any of the time periods.Rather it depends on how far they are apart, which is determined by `\(k\)`. This is one of the primary features of stationary time series. 	

---

## Autocorrelation function 

The autocorrelation function can be defined as the following: 

.pull-left[

`$$\begin{equation}
 \begin{split}
 \rho_k &amp;= \frac{Cov(Y_t,Y_{t-k})}{\sqrt{Var(Y_t)} \sqrt{Var(Y_{t-k})}} \\
       &amp;= \frac{Cov(Y_t,Y_{t-k})}{\sqrt{Var(Y_t)} \sqrt{Var(Y_t)}} \because  Var(y_t)=Var(Y_{t-k}) \\
       &amp;= \frac{Cov(Y_t,Y_{t-k})}{Var(Y_t)} \\ 
       &amp;=\frac{\theta^k*\frac{\sigma^2}{1-\theta^2}}{\frac{\sigma^2}{1-\theta^2}} \\
\rho_k &amp;=\theta^k
 \end{split}
\end{equation}$$`

]
.pull-right[

Alternative formulation

`$$\begin{equation}
 \begin{split}
 \rho_k &amp;= \frac{Cov(Y_t,Y_{t-k})}{\sqrt{Var(Y_t)} \sqrt{Var(Y_{t-k})}} \\
       &amp;= \frac{Cov(Y_t,Y_{t-k})}{\sqrt{Var(Y_t)} \sqrt{Var(Y_t)}} \because  Var(y_t)=Var(Y_{t-k}) \\
       &amp;= \frac{\gamma_k}{\gamma_0} \\ 
       &amp;=\frac{\theta^k \gamma_0}{\gamma_0} \\
\rho_k &amp;=\theta^k
 \end{split}
\end{equation}$$`

]


---

## R Simulation


```r
rho &lt;- function(k,alpha) alpha^k
```

.pull-left[


```r
y &lt;- rho(0:10, 0.7)
plot(0:10, y, type="b")
```

&lt;img src="DU_TSA_lecture_slides_files/figure-html/unnamed-chunk-9-1.png" width="100%" style="display: block; margin: auto;" /&gt;
`$$y_t=0.7y_{t-1}+e_t$$`

]
.pull-right[


```r
x &lt;- rho(0:10, -0.7)
plot(0:10, y, type="b")
```

&lt;img src="DU_TSA_lecture_slides_files/figure-html/unnamed-chunk-10-1.png" width="100%" style="display: block; margin: auto;" /&gt;
`$$y_t=-0.7y_{t-1}+e_t$$`
]


---

## AR(1) simulation

Let's simulate AR(1)


```r
set.seed(1)
y.ar &lt;- e &lt;- rnorm(100)
for (t in 2:100) y.ar[t]  &lt;- 0.7 * y.ar[t-1] +e[t]
```



```r
plot(y.ar,type="l")            
```

&lt;img src="DU_TSA_lecture_slides_files/figure-html/unnamed-chunk-12-1.png" width="70%" style="display: block; margin: auto;" /&gt;

---

.pull-left[


```r
acf(y.ar)
```

&lt;img src="DU_TSA_lecture_slides_files/figure-html/unnamed-chunk-13-1.png" width="100%" style="display: block; margin: auto;" /&gt;
           
]
.pull-right[


```r
pacf(y.ar)
```

&lt;img src="DU_TSA_lecture_slides_files/figure-html/unnamed-chunk-14-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]

---

## Model fitted to simulated AR(1)

Let's use the `ar()` function to our simulated data 


```r
y.ar.mod &lt;- ar(y.ar, method="mle")
y.ar.mod$order
```

```
[1] 1
```
The model correctly find the right order 1


```r
y.ar.mod$ar
```

```
[1] 0.6009459
```

Let's find the confidence interval for the coefficient


```r
y.ar.mod$ar + c(-2,2) * sqrt(y.ar.mod$asy.var)
```

```
[1] 0.4404031 0.7614886
```


```r
y.ar.ts &lt;- ts(y.ar, start=1980, frequency = 12)
auto.arima(y.ar.ts)
```

```
Series: y.ar.ts 
ARIMA(1,0,0) with non-zero mean 

Coefficients:
         ar1    mean
      0.6010  0.3544
s.e.  0.0808  0.2196

sigma^2 estimated as 0.8042:  log likelihood=-130.21
AIC=266.42   AICc=266.67   BIC=274.24
```
---




class: inverse,middle,center

# Moving Average Models

---

## Introduction

Another very simple time series model is moving average of order 1 or MA(1). This process is given by:

`$$\begin{equation}
    Y_t=\mu+\varepsilon_t+\alpha \varepsilon_{t-1}
\end{equation}$$`

In above equation, `\(Y_t\)` is sum of constant mean plus weighted average of current and past error. Usually weighted average is in the form of `\(\alpha \varepsilon+(1-\alpha) \varepsilon\)` form. I have to look further into it. Basically the values of `\(Y_t\)` are defined in terms of drawings from White Noise processes `\(\varepsilon_t\)`. 

---

## Mean of MA(1)

Mean of `\(MA(1)\)` process is pretty simple: 

`$$\begin{equation}
    E(Y_t)=\mu \quad \because E(\varepsilon_t)=E(\varepsilon_{t-1})=0
\end{equation}$$`

## Variance of MA(1)

`$$\begin{equation}
    \begin{split}
	Var(Y_t) &amp; =E[Y_t-E(Y_t)]^2\\
	&amp; =E(\cancel{\mu}+\varepsilon_t+\alpha \varepsilon_{t-1}-\cancel{\mu})^2\\
	&amp; =E(\varepsilon_t+\alpha \varepsilon_{t-1})^2\\
	&amp; =E(\varepsilon_t)^2+\alpha^2 E(\varepsilon_{t-1}^2)\\
	&amp; =\sigma^2+\alpha^2 \sigma^2\\ 
	&amp; =\sigma^2(1+\alpha^2)
  \end{split}
\end{equation}$$`

---

## Covariance of MA process

`$$\begin{equation}
    \begin{split}
	Cov(Y_t,Y_{t-1}) &amp; = E[Y_t-E(Y_t)][Y_{t-1}-E(Y_{t-1})]\\
		     &amp; = E(\varepsilon_t+\alpha \varepsilon_{t-1})(\varepsilon_{t-1}+\alpha \varepsilon_{t-2})\\
		     &amp; =\alpha E(\varepsilon_{t-1}^2) \quad \because \quad Cov(\varepsilon_t,\varepsilon_{t-k})=0 \quad \forall \,t \quad \text{when} \quad k\neq 0\\ 
		     &amp; = \alpha \sigma^2 \\
	    Cov(Y_t, Y_{t-2}) &amp; = E[Y_t-E(Y_t)][Y_{t-2}-E(Y_{t-2})]\\
		     &amp; = E(\varepsilon_t+\alpha \varepsilon_{t-1})(\varepsilon_{t-2}+\alpha \varepsilon_{t-3})\\
		     &amp; = 0 \quad \because \text{all cross covariance of error terms are zero}\\ 
	Similarly Cov(Y_t,Y_{t-k}) &amp; = 0 \quad \forall \quad k\ge 2
			\end{split}
			\label{eqmacov}
\end{equation}$$`

The equation above implies that `\(AR(1)\)` and `\(MA(1)\)` has very different autocovariance structure. 

---

## MA(1) simulation

Let's simulate MA(1)


```r
set.seed(1)
x.ma &lt;- w &lt;- rnorm(100)
for (t in 2:100) x.ma[t]  &lt;- w[t] + 0.7 * w[t-1] 
```



```r
plot(x.ma,type="l")            
```

&lt;img src="DU_TSA_lecture_slides_files/figure-html/unnamed-chunk-20-1.png" width="70%" style="display: block; margin: auto;" /&gt;

---

.pull-left[


```r
acf(x.ma)
```

&lt;img src="DU_TSA_lecture_slides_files/figure-html/unnamed-chunk-21-1.png" width="70%" style="display: block; margin: auto;" /&gt;
           
]
.pull-right[


```r
pacf(x.ma)
```

&lt;img src="DU_TSA_lecture_slides_files/figure-html/unnamed-chunk-22-1.png" width="70%" style="display: block; margin: auto;" /&gt;
]

---

## Model fitted to simulated MA(1)


```r
x.ma.ts &lt;- ts(x.ma, st=1980, frequency=12)
plot(x.ma.ts)
```

&lt;img src="DU_TSA_lecture_slides_files/figure-html/unnamed-chunk-23-1.png" width="70%" style="display: block; margin: auto;" /&gt;

---


```r
library(forecast)
auto.arima(x.ma.ts)
```

```
Series: x.ma.ts 
ARIMA(0,0,1) with zero mean 

Coefficients:
         ma1
      0.7081
s.e.  0.0736

sigma^2 estimated as 0.8135:  log likelihood=-131.42
AIC=266.84   AICc=266.96   BIC=272.05
```


---

class: inverse,middle,center 

# MA(q) process: Definition and properties 

---

## MA process

A moving average (MA) process of order `\(q\)` is a linear combination of the current white noise term

The q most recent past white noise terms and is defined by
`$$y_t=e_t+\beta_1e_{t-1}+\ldots+\beta_1e_{t-q}$$` 

Here `\({e_t}\)` is white noise with zero mean and variance `\(\sigma_e^2\)`. 

---


## MA equation with backshift operator

The above equation can be rewritten in terms of the backward shift operator `B`

`$$y_t=(1+\beta_1B+\beta_2B^2+\ldots+\beta_qB^q)e_t=\phi_q(B)w_t$$` 

Here `\(\phi_q\)` is a polynomial of order `\(q\)`. 

MA processes consist of a finite sum stationary white noise terms 

They are stationary and hence have a time-invariant mean and autocovariance 

---

## Mean and variance of MA(q) process

The mean and variance for `\({x_t}\)` are easy to derive 

The mean is just zero because it is a sum of terms that all have a mean of zero 

The variance is `\(\sigma{^2}_{e}(1+\beta_1^2+\ldots+\beta^2_q)\)` 

Each of the white noise terms has the same variance and the terms are mutually independent. 

---

## ACF of MA(q) process

* As we have previously seen, `Autocorrelation` as a function of lag order `\((k)\)` is called `\(autocorrelation function\)` or `\(ACF\)` 
* ACF plays a major role in understanding a time series. 
* It tells us how deep is the memory of the underlying stochastic process. 
* If it has long memory, as in the case of `\(AR(1)\)`, we would see that value of `\(\rho_k\)` does not diminish to zero with increasing value of `\(k\)` as quickly as the `\(MA(1)\)` does.
* We already have seen previously that covariance between two periods, in a `\(MA(1)\)` process, reduces to zero if  lag is just more than one period. 
* Therefore by looking into the ACF we can have a fair idea about the underlying time series stochastic process. 


---

In the case of `\(MA(1)\)`

`$$\begin{align}
	\gamma_1 &amp; =Cov(Y_t,Y_{t-1})=\alpha \sigma^2,\quad 
	\text{and}\\
	\gamma_0 &amp; =Var(Y_t)= \sigma^2 (1+\alpha^2) \\
	\text{Therefore, from the above two we can write}
	\rho_1 &amp; =\frac{\gamma_1}{\gamma_0} \\
	&amp;=\frac{\alpha \cancel{\sigma^2}}{\cancel{\sigma^2}(1+\alpha^2)} 
	&amp;=\frac{\alpha}{1+\alpha^2}
	\label{}
\end{align}$$`

And in the case of $k \ge 2 $, we have also seen that `\(\gamma_k=0 \quad \forall k\)`. Therefore,

`$$\begin{equation}
	\rho_k=0 \quad \forall \quad k
	\label{}
\end{equation}$$`


Implication of the above derivation is that, a shock in `\(MA(1)\)` will only last two period, `\(Y_t\)` and `\(Y_{t-1}\)` while  a shock in `\(AR(1)\)` will affect all future observations with a decreasing effect, since `\(|\theta|&lt;1\)`.

---


## Comparing AR(1) and MA(1)

We can generalize `\(AR(1)\)` and `\(MA(1)\)` by adding additional lag terms. In general, there is little difference between these two models. We express$\mathbf{AR(1)}$  as$\mathbf{MA(1)}$ by repeated substitution.We can rewrite \texttt{AR(1)} as an infinite order of moving average. We can see this in the following:

`$$\begin{align}
	Y_t &amp;=\delta+\theta Y_{t-1}+\varepsilon \\
	&amp;=\delta+\theta [\delta+\theta Y_{t-2}+\varepsilon_{t-1}]+\varepsilon_t\\
	&amp;=\delta+\theta \delta+\theta^2 Y_{t-2}+\theta \varepsilon_{t-1}+\varepsilon_ti \label{del}\\
\text{We also have previously found that}\\
\mu &amp;=\frac{\delta}{1-\theta}\\
\text{or,}\quad \delta &amp;=\mu (1-\theta)\\
\text{Now putting this back into equation}\\ 
	&amp;=\mu (1-\theta)+\theta \mu (1-\theta)+\theta^2 Y_{t-2}+\theta \varepsilon_{t-1}+\varepsilon_t\\
	&amp;=\mu - \mu \theta+ \theta \mu -\mu \theta^2+\theta^2 Y_{t-2}+\theta \varepsilon_{t-1}+\varepsilon_t\\
	&amp;=\mu - \cancel{\mu \theta}+ \cancel{\theta \mu} -\mu \theta^2+\theta^2 Y_{t-2}+\theta \varepsilon_{t-1}+\varepsilon_t\\
	&amp;=\mu+\theta^2(Y_{t-2}-\mu)+\theta \varepsilon_{t-1}+\varepsilon_t
\end{align}$$`

---

`$$\begin{align}
	\text{Similarly, by substituting for}\, Y_{t-2}, \text{we get}\\
	&amp;=\mu+\theta^3(Y_{t-3}-\mu)+\varepsilon_t+\varepsilon_t+\theta \varepsilon_{t-1}+\theta^2 \varepsilon_{t-2}\\
	&amp;\vdots\\
    &amp;=\mu+\theta^n(Y_{t-n}-\mu)+\sum_{j=0}^{n-1}\theta^j \varepsilon_{t-j}\\
\end{align}$$`

When `\(n\longrightarrow \infty\)` and `\(\theta &lt;1\)` (remember the stationarity condition) , above equation boils down to 

`$$\begin{equation}
		Y_t=\mu+\sum_{j=0}^{n-1}\theta^j \varepsilon_{t-j}\\
		\label{eqtrar}
\end{equation}$$`

In the same manner, we can try to see whether an `\(\texttt{MA(1)}\)` process can be transformed into some kind of `\(\texttt{AR}\)` process. 

`$$\begin{align}
		MA(1) &amp; =\mu+\varepsilon_t+\varepsilon_{t-1}\\
		&amp; = \mu+Y_t-\delta - \theta Y_{t-1}+Y_{t-1}-\delta-\theta Y_{t-2} \\
		&amp;=\frac{\delta}{1-\theta}-2 \delta+(1-\theta)Y_{t-1}-\theta Y_{t-2} \\
		&amp;=\sigma_0+\sigma_1 Y_{t-1}+\sigma_2 Y_{t-2}
		\label{}
\end{align}$$`

---








    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:10",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
