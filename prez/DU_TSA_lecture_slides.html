<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>TIME SERIES LECTURES</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs-2.3/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/hygge.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/ninjutsu.css" rel="stylesheet" />
    <link rel="stylesheet" href="myprez.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# TIME SERIES LECTURES

---










## Stationarity 

--

A stochastic process is said to be stationary if it's properties are unaffected by a change of time origin.

--

Time origin might mean the particular time period which represents the time range. 

--

It might be the starting time period or the center value of time. 

--

In other words, joint probability distribution of any set of times does not change shape or affected by any means if there is a shift in the time axis. 

--

Why we talk about joint probability distribution here?  

--

This is because every value in every time period is a particular value of a random variable. 

--

For example, `\(Y_t\)` is random variable, same as `\(Y_{t-1}\)` is a random variable.

--

When a time-series consists of these two variables, the distribution of this time series is a joint distribution.


---

## Strict stationarity

--

A time series model `\({x_t}\)` is strictly stationary if the joint distribution of `\(x_1,\ldots,x_{t_n}\)` is the same as the joint distribution of `\({x_{1+m},\ldots,x_{t+m}}\)` for all `\(t_1,\ldots,t_n\)` and m, so that the distribution is unchanged after an arbitrary time shift.

--

Strict stationarity implies that the mean and variance are constant in time

--

Autocovariance `\(Cov(x_t, x_s)\)` only depends on lag `\(k=|t-s|\)` and can be written `\(\gamma(k)\)` 

---

## Weak stationarity

--

If a series is not not strictly stationary but the mean and variance are constant in time and autocovariance only depends on the lag, then the series is called second-order stationary.

--

Usually stationarity is a very strict condition which requires that the whole distribution must be unchanged at any time period. 

--

It means that all the moments in any order should be equal. 

--

But this is usually impractical to find. 

--

As a result, we impose a softer version of this \textbf{strict stationarity} which we call \textbf{weak stationarity}. 

--

Weak stationarity is a situation when we have just mean, variance are equal. 

---

## Covariance stationary


We also have covariance which does not change we time difference between variables are the same. This means:


`$$\begin{equation}
    Cov(Y_1,Y_5)=Cov(Y_{17},Y_{21})
\end{equation}$$`


 This is also called covariance stationary. 

--

 We can formally express the covariance stationary in the following way:

`$$\begin{align}
	E(Y_t)&amp;=\mu &lt; \infty \\ 
	Var(Y_t)&amp;=\sigma^2 \\
	Cov(Y_t,Y_{t-k})&amp;=E[(Y_t-E(Y_{t-1}))(Y_{t-k}-E(Y_{t-k}))] \\ 
	&amp;=E[(Y_t-\mu)(Y_{t-k}-\mu)]\\
	&amp;=\gamma_k \label{sco} \quad k=1,2,3,\dotsc
\end{align}$$`

In the above, we find that,  weakly stationary process is characterized by constant finite mean and variance. 


It also imply that covariance between any two time periods does not depend on the time period they are in, rather it depends on the time lag between these two variables. 

???

If the white noise is Gaussian, the stochastic process is commpletely defined by the mean and covariance structure, in the same way as any normal distribution is defined by its mean and variance-covariance matrix.

First step in any should be to check whether there is any evidence of a trend or seasonal effects

If there is we will have to remove them

It is often reasonable to treat the time series of residuals as a realisation of a stationary error series.

Therefore, these models in this topic are often fitted to residual series arising from regression analyses

---

## Autocorrelation function (ACF)

--

**Autocovariance**

--

Since here we are trying to find covariance between two variables where one is the lag version of the same variable, we call these `\(\textbf{autocovariances}\)`. 

Here we define the `\(\textbf{k-th order autocovariance}\)` as:

`$$\begin{equation}
    \gamma_k=Cov(Y_t,Y_{t-k})=Cov(Y_{t-k},Y_t)
\end{equation}$$`

When `\(k=0\)`, `\(\gamma_k\)` reduces to:

`$$\begin{equation}
    \gamma_0=Cov(Y_t,Y_t)=Var(Y_t)
    \label{gam0}
\end{equation}$$`

---

## Autocorrelation		

--

One problem with covariance its value is not independent of the units in which the variables are measured. 

--

For example, `\(Cov(Y_1,Y_5)\)`  and `\(Cov(Y_12, Y_16)\)` might not equal just because units are different. 

--

For example, let's say `\(Y_t\)` is measuring the CPI. In early time periods, CPI usually had lower values but suddenly it got a shift in later time periods. 

--

It's better to use correlation measure which is unit free, obviously we will use `\(\textbf{autocorrelation}\)` which standardizes to compare across different series. 

--

Here we will derive `\(\rho_k\)` which is the k-th order autocorreation:

`$$\begin{gather}
    \rho_k &amp; =\frac{Cov(Y_t,Y_{t-k})}{\sqrt{Var(Y_t)}\sqrt{Var(Y_{t-1})}} \\
    &amp; =\frac{\gamma_k}{\gamma_0} \quad (\text{by}\quad \eqref{gam0})
\end{gather}$$`

--

Obviously, here `\(\rho_0=\frac{Cov(Y_t,Y_t)}{\gamma_0}=\frac{Var(Y_t)}{\gamma_0}=\frac{\gamma_0}{\gamma_0}=1\)` 

And also as usual, `\(-1\le \rho_k \le 1\)` which is standard for autocorrelation measures.

---



class: inverse,center,middle

# Autogregressive models

---

## Specification

We will start with the simplest form of time-series model which is called first-order autoregressive models or AR(1).

--

`$$\begin{equation}
    Y_t=\alpha+\theta Y_{t-1}+\varepsilon_t
\end{equation}$$`

--

A simple way to model dependence between observations in different time periods would be that `\(Y_t\)` depends linearly on the observation from the previous time period `\(Y_{t-1}\)`.

--

* Here `\(\varepsilon_t\)` means serially uncorrelated innovation with mean of zero and a constant variance. 
--

* The process in the above equation is called a first order `\(\textbf{autoregressive process}\)` or `\(AR(1)\)` process. 

--

* This process tells us that the value of Y at time `\(t\)` depends on a constant term plus `\(\theta\)` times plus an unpredictable component `\(\varepsilon\)`. 

--

* Here we shall assume `\(|\theta|&lt;1\)`.  

--

* The process underlying `\(\varepsilon_t\)`  called `\(\textbf{white noise process}\)`. 

--

* Here `\(\varepsilon\)` will be always homoskedastic and will not show any autocorrelation. 

---

## Expected Value of AR(1)

The expected value of `\(Y_t\)` can be solved from 

`$$\begin{equation*} E(Y_t)=\delta+\theta E(Y_{t-1}) \end{equation*}$$`

--

which, assuming that `\(E(Y_t)\)` does not depend upon `\(t\)`, allows us to write 

--

`$$E(Y_t)=E(Y_{t-1})=\mu \label{}$$`

--

Using the above assumption, 

--

`$$\begin{equation}
    \mu=\delta+\theta\mu \\
    \mu - \theta \mu = \delta \\
    \mu(1-\theta)= \delta \\
    \mu =\frac{\delta}{1-\theta}
\end{equation}$$`

--

Remember, this is only true if `\(Y_t\)` is stationary, that means it's statistical property does not depend on a particular time period, in other words, the mean is constant. We will also write `\(y_t=Y_t-\mu\)`. Now it follows that 

---

Continuing on,

--

`$$\begin{gather*}
    y_t+\mu=\delta+\theta (y_{t-1}+\mu)+\varepsilon \\ 
    \text{In the above we have seen that} \\ 
    \mu\equiv E(Y_t)=\frac{\delta}{(1-\theta)} \\ 
    \text{which means} \\ 
    \delta=\mu (1-\theta) \\ 
    \text{Now putting this value in the above equation} \\ 
    y_t+\mu=\mu (1-\theta)+\theta y_{t-1}+\theta \mu+\varepsilon \\ y_t+\cancel{\mu}=\cancel{\mu}-\mu \theta+\theta y_{t-1}+\theta \mu+\varepsilon \\ 
    y_t=\cancel{\mu \theta}+\theta y_{t-1}+\cancel{\theta \mu}+\varepsilon \\ 
    y_t=\theta y_{t-1}+\varepsilon 
\end{gather*}$$`

???

Defining time series models in terms of `\(y_t\)` rather than `\(Y_t\)` is often notationally convenient. So `\(y_t\)` has been introduced so that depiction of time models remain concise

---

Taking expectation, we get :
`$$E(y_t)=\theta E(y_{t-1})+ E(\varepsilon)$$` 

--

Since `\(E(\varepsilon)=0\)` we can write 
`$$E(y_t)=\theta E(y_{t-1})$$` 

--

Remember, previously we said that `\(Y_t\)` is stationary and as a result `\(E(Y_t)=E(Y_{t-1})\)`

--

Then we can write,

--

`$$\begin{gather*}
    E(y_t+\mu) =E(y_{t-1}+\mu) \\
    E(y_t)+E(\mu)=E(y_{t-1})+E(\mu) \\
 \text{since}\,\mu\, \text{is a constant}\,E(\mu)=\mu  \\
    E(y_t)+\cancel{\mu}=E(y_{t-1})+\cancel{\mu} \\
    E(y_t)=E(y_{t-1}) \\
    \text{Now from the above, we can write} \\
    E(y_t)-\theta E(y_t)=0 \quad \text{since} \quad E(y_t)=E(y_{t-1}) \\
    E(y_t)=0
\end{gather*}$$`

Exaplanation: 

`$$E(y_t) (1-\delta) = 0$$`

If   `\(\delta\neq 1\)` then `\(1-\delta\neq 0\)`
If `\(1-\delta\neq 0\)`   then `\(E(y_t) =0\)`  


---

* Above results show that `\(y_t\)` has a zero mean. But to have a non-zero mean we can add an intercept. 

--

* We also here note that `\(V(y_t)=V(y_t)\)`. 

--

* The process described in above equation imposes certain restrictions on the stochastic process that generates `\(Y_t\)`. 

--

* Usually when we have a group of variables, we usually describe their joint distribution by  covariances. 

--

* Since, here are lagged version of other variables, we call it `\(\texttt{autocovariances}\)` .  

--

* This is the so called `\(\texttt{stationarity}\)` condition. 

--

* Basically we require stationarity to derive dynamic properties of `\(Y_t\)`.

--

* Without deriving dynamic properties of `\(Y_t\)`, we can not forecast its values. 

---

## Variance of AR(1)

Previously we derived the constant mean of the distribution by imposing that mean does not depend on `\(t\)`. Now we will do the same thing with variances. 

--

First let's derive the variance:


--

`$$\begin{equation*}
	\begin{split}
		Var(Y_t)&amp;=Var(\delta+\theta Y_{t-1}+\varepsilon_t)\\
		 &amp;=Var(\delta)+Var(\theta Y_{t-1}+\varepsilon_t) \\
		&amp;=Var(\theta Y_{t-1}+\varepsilon_t) \quad \because \quad Var(\delta)=0\\
		&amp;=\theta^2 Var(Y_{t-1})+Var(\varepsilon_t) 
	\end{split}
\end{equation*}$$`

--

Therefore, we can write

    
Here we assume that `\(Cov(Y_{t-1} \varepsilon_t)=0\)`. Now, this is not too unrealistic in the sense that error at current period might not be correlated with the endogenous variable in the past.Now it's time to impose one of the stationarity condition, namely, variance of the time series process does not depend on time:

`$$\begin{equation}\label{eqst}
		Var(Y_t)=Var(Y_{t-1})
	\end{equation}$$`

---

Now using above equations,

`$$\begin{gather}
    Var(Y_t) =\theta^2 Var(Y_t)+ \sigma^2 \quad \text{by} \quad Var(\varepsilon_t)=\sigma^2\\
		\text{or,} \quad Var(Y_t)(1-\theta^2) =\sigma^2\\
		\text{or,} \quad Var(Y_t) =\frac{\sigma}{1-\theta^2} \\
		\text{or,}   \gamma_0 = \frac{\sigma}{1-\theta^2}
\end{gather}$$`


We see from the above equation  that `\(Var(Y_t)\)` is indeed constant. We also see from above that we can only impose `\(Var(Y_t)=Var(Y_{t+1}\)` if `\(|\theta|&lt;1\)` . This is the assumption we madel previously.This is actually the essence of `\(\texttt{Dickey-Fuller}\)` test which tests whether this coefficient is less than one or not. 

---

## Covariances of AR(1)	

Now let's find the `\(Covariance\)` between `\(Y_t\)` and `\(Y_{t-1}\)`

`$$\begin{equation} 
    \begin{split} 
	Cov(Y_t,Y_{t-1}) &amp; =E(Y_t-E(Y_t))E(Y_{t-1}-E(Y_{t-1})) \\ 
		     &amp; =E((Y_t-\mu)(Y_{t-1}-\mu))  \quad \because \quad E(Y_t)=E(Y_{t-1})=\mu \\
		     &amp; =E(y_t y_{t-1}) \quad \text{by the definition of}\quad y_t\\
		     &amp; =E((\theta y_{t-1}+\varepsilon_t)(y_{t-1})) \because \, y_t=\theta y_{t-1}+\varepsilon_t \\
                     &amp; =\theta (E(y_{t-1})^2+E(\varepsilon_t y_{t-1}) \\
		     &amp; =\theta \, E(Y_{t-1}-\mu)^2+E(\varepsilon_t y_{t-1})\\
		     &amp; =\theta \, Var(Y_t)+E((\varepsilon_t-E(\varepsilon_t)) (Y_{t-1}-\mu))\\
		     &amp; =\theta \, Var(Y_t)+Cov(\varepsilon_t,Y_t)\\
		     &amp; =\theta \, Var(Y_t)\quad \because \quad Cov(\varepsilon_t,Y_{t-1})=0
		\label{eqcov}
    \end{split} 
\end{equation}$$`

So, we have established that 

`$$\begin{equation}
	Cov(Y_t,Y_{t-1})=\gamma_1=\theta \frac{\sigma^2}{1-\theta^2}
\end{equation}$$`

---

Now let's take some higher order lags. Let's see what would be the covariance between `\(Y_t\)` and `\(Y_{t-2}\)`, that is autocovariance of lag order 2.


`$$\begin{equation}
		\begin{split}
		Cov(Y_t,Y_{t-2}) &amp; =E(Y_t-E(Y_t))E(Y_{t-2}-E(Y_{t-2}))\\
		&amp; =E((Y_t-\mu)(Y_{t-2}-\mu))\quad \because \quad E(Y_t)=E(Y_{t-2})=\mu\\
		&amp; =E(y_t\, y_{t-2})\quad \text{by the definition of} Y_t\\
\text{Before continuing we have to figure out} \, y_t\\
		y_t &amp; =\theta y_{t-1}+\varepsilon_t\\
		&amp; =\theta (\theta y_{t-2}+\varepsilon_{t-1})+\varepsilon_t\\
		&amp; =\theta^2 y_{t-2}+\theta \varepsilon_{t-1}+\varepsilon_t\\
\text{Now,putting this value back in equation}
		&amp; =E((\theta^2 y_{t-2}+\theta \varepsilon_{t-1}+\varepsilon_t)y_{t-2})\\
		&amp; =\theta^2 (E(y_{t-2})^2+E(\varepsilon_{t-1} y_{t-2})+E(\varepsilon_t y_{t-2}))\\
		&amp; =\theta^2 \, E(Y_{t-2}-\mu)^2+Cov(\varepsilon_{t-1},y_{t-2})+Cov(\varepsilon_t,y_{t-2})\\
		&amp; =\theta^2 \, Var(Y_t)\quad \because \quad Cov(\varepsilon_t,Y_{t-k})=0\\
		&amp; =\theta^2 \, \frac{\sigma^2}{1-\theta^2}
		\end{split}
\end{equation}$$`

---

`$$\begin{equation}
    \text{Therefore we see that,} \\
     Cov(Y_t,Y_{t-1}) = \gamma_1=\theta \frac{\sigma^2}{1-\theta^2} = \theta \gamma_0\\
     Cov(Y_t,Y_{t-2}) =\gamma_2=\theta^2 \frac{\sigma^2}{1-\theta^2} = \theta^2 \gamma_0\\
    \vdots \\
     Cov(Y_t,Y_{t-k}) =\gamma_k=\theta^k \frac{\sigma^2}{1-\theta^2} = \theta^k \gamma_0
\end{equation}$$`

We have the following observation from the above Covariance formula


* As long as `\(\theta\)` is non-zero, any two observation on `\(Y\)` has non-zero correlation. This might imply that as long as `\(Y\)` in period `\(t\)` is correlated with it's immediate past value, that correlation carries through higher degree of past value.
* Since the value of `\(|\theta|&lt;1\)`, this correlation diminishes in higher degree of lag-order. This is quite intuitive as itis natural to have weakerza correlation among values whic are far apart. 
* Covariance between any two time periods does not depend on any of the time periods.Rather it depends on how far they are apart, which is determined by `\(k\)`. This is one of the primary features of stationary time series. 	

---

## Autocorrelation function 

The autocorrelation function can be defined as the following: 

.pull-left[

`$$\begin{equation}
 \begin{split}
 \rho_k &amp;= \frac{Cov(Y_t,Y_{t-k})}{\sqrt{Var(Y_t)} \sqrt{Var(Y_{t-k})}} \\
       &amp;= \frac{Cov(Y_t,Y_{t-k})}{\sqrt{Var(Y_t)} \sqrt{Var(Y_t)}} \because  Var(y_t)=Var(Y_{t-k}) \\
       &amp;= \frac{Cov(Y_t,Y_{t-k})}{Var(Y_t)} \\ 
       &amp;=\frac{\theta^k*\frac{\sigma^2}{1-\theta^2}}{\frac{\sigma^2}{1-\theta^2}} \\
\rho_k &amp;=\theta^k
 \end{split}
\end{equation}$$`

]
.pull-right[

Alternative formulation

`$$\begin{equation}
 \begin{split}
 \rho_k &amp;= \frac{Cov(Y_t,Y_{t-k})}{\sqrt{Var(Y_t)} \sqrt{Var(Y_{t-k})}} \\
       &amp;= \frac{Cov(Y_t,Y_{t-k})}{\sqrt{Var(Y_t)} \sqrt{Var(Y_t)}} \because  Var(y_t)=Var(Y_{t-k}) \\
       &amp;= \frac{\gamma_k}{\gamma_0} \\ 
       &amp;=\frac{\theta^k \gamma_0}{\gamma_0} \\
\rho_k &amp;=\theta^k
 \end{split}
\end{equation}$$`

]


---

## R Simulation


```r
rho &lt;- function(k,alpha) alpha^k
```

.pull-left[


```r
y &lt;- rho(0:10, 0.7)
plot(0:10, y, type="b")
```

&lt;img src="DU_TSA_lecture_slides_files/figure-html/unnamed-chunk-9-1.png" width="100%" style="display: block; margin: auto;" /&gt;
`$$y_t=0.7y_{t-1}+e_t$$`

]
.pull-right[


```r
x &lt;- rho(0:10, -0.7)
plot(0:10, y, type="b")
```

&lt;img src="DU_TSA_lecture_slides_files/figure-html/unnamed-chunk-10-1.png" width="100%" style="display: block; margin: auto;" /&gt;
`$$y_t=-0.7y_{t-1}+e_t$$`
]


---

## AR(1) simulation

Let's simulate AR(1)


```r
set.seed(1)
y.ar &lt;- e &lt;- rnorm(100)
for (t in 2:100) y.ar[t]  &lt;- 0.7 * y.ar[t-1] +e[t]
```



```r
plot(y.ar,type="l")            
```

&lt;img src="DU_TSA_lecture_slides_files/figure-html/unnamed-chunk-12-1.png" width="70%" style="display: block; margin: auto;" /&gt;

---

.pull-left[


```r
acf(y.ar)
```

&lt;img src="DU_TSA_lecture_slides_files/figure-html/unnamed-chunk-13-1.png" width="100%" style="display: block; margin: auto;" /&gt;
           
]
.pull-right[


```r
pacf(y.ar)
```

&lt;img src="DU_TSA_lecture_slides_files/figure-html/unnamed-chunk-14-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]

---

## Model fitted to simulated AR(1)

Let's use the `ar()` function to our simulated data 


```r
y.ar.mod &lt;- ar(y.ar, method="mle")
y.ar.mod$order
```

```
[1] 1
```
The model correctly find the right order 1


```r
y.ar.mod$ar
```

```
[1] 0.6009459
```

Let's find the confidence interval for the coefficient


```r
y.ar.mod$ar + c(-2,2) * sqrt(y.ar.mod$asy.var)
```

```
[1] 0.4404031 0.7614886
```


```r
y.ar.ts &lt;- ts(y.ar, start=1980, frequency = 12)
auto.arima(y.ar.ts)
```

```
Series: y.ar.ts 
ARIMA(1,0,0) with non-zero mean 

Coefficients:
         ar1    mean
      0.6010  0.3544
s.e.  0.0808  0.2196

sigma^2 estimated as 0.8042:  log likelihood=-130.21
AIC=266.42   AICc=266.67   BIC=274.24
```
---




class: inverse,middle,center

# Moving Average Models

---

## Introduction

Another very simple time series model is moving average of order 1 or MA(1). This process is given by:

`$$\begin{equation}
    Y_t=\mu+\varepsilon_t+\alpha \varepsilon_{t-1}
\end{equation}$$`

In above equation, `\(Y_t\)` is sum of constant mean plus weighted average of current and past error. Usually weighted average is in the form of `\(\alpha \varepsilon+(1-\alpha) \varepsilon\)` form. I have to look further into it. Basically the values of `\(Y_t\)` are defined in terms of drawings from White Noise processes `\(\varepsilon_t\)`. 

---

## Mean of MA(1)

Mean of `\(MA(1)\)` process is pretty simple: 

`$$\begin{equation}
    E(Y_t)=\mu \quad \because E(\varepsilon_t)=E(\varepsilon_{t-1})=0
\end{equation}$$`

## Variance of MA(1)

`$$\begin{equation}
    \begin{split}
	Var(Y_t) &amp; =E[Y_t-E(Y_t)]^2\\
	&amp; =E(\cancel{\mu}+\varepsilon_t+\alpha \varepsilon_{t-1}-\cancel{\mu})^2\\
	&amp; =E(\varepsilon_t+\alpha \varepsilon_{t-1})^2\\
	&amp; =E(\varepsilon_t)^2+\alpha^2 E(\varepsilon_{t-1}^2)\\
	&amp; =\sigma^2+\alpha^2 \sigma^2\\ 
	&amp; =\sigma^2(1+\alpha^2)
  \end{split}
\end{equation}$$`

---

## Covariance of MA process

`$$\begin{equation}
    \begin{split}
	Cov(Y_t,Y_{t-1}) &amp; = E[Y_t-E(Y_t)][Y_{t-1}-E(Y_{t-1})]\\
		     &amp; = E(\varepsilon_t+\alpha \varepsilon_{t-1})(\varepsilon_{t-1}+\alpha \varepsilon_{t-2})\\
		     &amp; =\alpha E(\varepsilon_{t-1}^2) \quad \because \quad Cov(\varepsilon_t,\varepsilon_{t-k})=0 \quad \forall \,t \quad \text{when} \quad k\neq 0\\ 
		     &amp; = \alpha \sigma^2 \\
	    Cov(Y_t, Y_{t-2}) &amp; = E[Y_t-E(Y_t)][Y_{t-2}-E(Y_{t-2})]\\
		     &amp; = E(\varepsilon_t+\alpha \varepsilon_{t-1})(\varepsilon_{t-2}+\alpha \varepsilon_{t-3})\\
		     &amp; = 0 \quad \because \text{all cross covariance of error terms are zero}\\ 
	Similarly Cov(Y_t,Y_{t-k}) &amp; = 0 \quad \forall \quad k\ge 2
			\end{split}
			\label{eqmacov}
\end{equation}$$`

The equation above implies that `\(AR(1)\)` and `\(MA(1)\)` has very different autocovariance structure. 

---

## MA(1) simulation

Let's simulate MA(1)


```r
set.seed(1)
x.ma &lt;- w &lt;- rnorm(100)
for (t in 2:100) x.ma[t]  &lt;- w[t] + 0.7 * w[t-1] 
```



```r
plot(x.ma,type="l")            
```

&lt;img src="DU_TSA_lecture_slides_files/figure-html/unnamed-chunk-20-1.png" width="70%" style="display: block; margin: auto;" /&gt;

---

.pull-left[


```r
acf(x.ma)
```

&lt;img src="DU_TSA_lecture_slides_files/figure-html/unnamed-chunk-21-1.png" width="70%" style="display: block; margin: auto;" /&gt;
           
]
.pull-right[


```r
pacf(x.ma)
```

&lt;img src="DU_TSA_lecture_slides_files/figure-html/unnamed-chunk-22-1.png" width="70%" style="display: block; margin: auto;" /&gt;
]

---

## Model fitted to simulated MA(1)


```r
x.ma.ts &lt;- ts(x.ma, st=1980, frequency=12)
plot(x.ma.ts)
```

&lt;img src="DU_TSA_lecture_slides_files/figure-html/unnamed-chunk-23-1.png" width="70%" style="display: block; margin: auto;" /&gt;

---


```r
library(forecast)
auto.arima(x.ma.ts)
```

```
Series: x.ma.ts 
ARIMA(0,0,1) with zero mean 

Coefficients:
         ma1
      0.7081
s.e.  0.0736

sigma^2 estimated as 0.8135:  log likelihood=-131.42
AIC=266.84   AICc=266.96   BIC=272.05
```


---

class: inverse,middle,center 

# MA(q) process: Definition and properties 

---

## MA process

A moving average (MA) process of order `\(q\)` is a linear combination of the current white noise term

The q most recent past white noise terms and is defined by
`$$y_t=e_t+\beta_1e_{t-1}+\ldots+\beta_1e_{t-q}$$` 

Here `\({e_t}\)` is white noise with zero mean and variance `\(\sigma_e^2\)`. 

---


## MA equation with backshift operator

The above equation can be rewritten in terms of the backward shift operator `B`

`$$y_t=(1+\beta_1B+\beta_2B^2+\ldots+\beta_qB^q)e_t=\phi_q(B)w_t$$` 

Here `\(\phi_q\)` is a polynomial of order `\(q\)`. 

MA processes consist of a finite sum stationary white noise terms 

They are stationary and hence have a time-invariant mean and autocovariance 

---

## Mean and variance of MA(q) process

The mean and variance for `\({x_t}\)` are easy to derive 

The mean is just zero because it is a sum of terms that all have a mean of zero 

The variance is `\(\sigma{^2}_{e}(1+\beta_1^2+\ldots+\beta^2_q)\)` 

Each of the white noise terms has the same variance and the terms are mutually independent. 

---

## ACF of MA(q) process

* As we have previously seen, `Autocorrelation` as a function of lag order `\((k)\)` is called `\(autocorrelation function\)` or `\(ACF\)` 
* ACF plays a major role in understanding a time series. 
* It tells us how deep is the memory of the underlying stochastic process. 
* If it has long memory, as in the case of `\(AR(1)\)`, we would see that value of `\(\rho_k\)` does not diminish to zero with increasing value of `\(k\)` as quickly as the `\(MA(1)\)` does.
* We already have seen previously that covariance between two periods, in a `\(MA(1)\)` process, reduces to zero if  lag is just more than one period. 
* Therefore by looking into the ACF we can have a fair idea about the underlying time series stochastic process. 


---

In the case of `\(MA(1)\)`

`$$\begin{align}
	\gamma_1 &amp; =Cov(Y_t,Y_{t-1})=\alpha \sigma^2,\quad 
	\text{and}\\
	\gamma_0 &amp; =Var(Y_t)= \sigma^2 (1+\alpha^2) \\
	\text{Therefore, from the above two we can write}
	\rho_1 &amp; =\frac{\gamma_1}{\gamma_0} \\
	&amp;=\frac{\alpha \cancel{\sigma^2}}{\cancel{\sigma^2}(1+\alpha^2)} 
	&amp;=\frac{\alpha}{1+\alpha^2}
	\label{}
\end{align}$$`

And in the case of $k \ge 2 $, we have also seen that `\(\gamma_k=0 \quad \forall k\)`. Therefore,

`$$\begin{equation}
	\rho_k=0 \quad \forall \quad k
	\label{}
\end{equation}$$`


Implication of the above derivation is that, a shock in `\(MA(1)\)` will only last two period, `\(Y_t\)` and `\(Y_{t-1}\)` while  a shock in `\(AR(1)\)` will affect all future observations with a decreasing effect, since `\(|\theta|&lt;1\)`.

---


## Comparing AR(1) and MA(1)

We can generalize `\(AR(1)\)` and `\(MA(1)\)` by adding additional lag terms. In general, there is little difference between these two models. We express$\mathbf{AR(1)}$  as$\mathbf{MA(1)}$ by repeated substitution.We can rewrite \texttt{AR(1)} as an infinite order of moving average. We can see this in the following:

`$$\begin{align}
	Y_t &amp;=\delta+\theta Y_{t-1}+\varepsilon \\
	&amp;=\delta+\theta [\delta+\theta Y_{t-2}+\varepsilon_{t-1}]+\varepsilon_t\\
	&amp;=\delta+\theta \delta+\theta^2 Y_{t-2}+\theta \varepsilon_{t-1}+\varepsilon_ti \label{del}\\
\text{We also have previously found that}\\
\mu &amp;=\frac{\delta}{1-\theta}\\
\text{or,}\quad \delta &amp;=\mu (1-\theta)\\
\text{Now putting this back into equation}\\ 
	&amp;=\mu (1-\theta)+\theta \mu (1-\theta)+\theta^2 Y_{t-2}+\theta \varepsilon_{t-1}+\varepsilon_t\\
	&amp;=\mu - \mu \theta+ \theta \mu -\mu \theta^2+\theta^2 Y_{t-2}+\theta \varepsilon_{t-1}+\varepsilon_t\\
	&amp;=\mu - \cancel{\mu \theta}+ \cancel{\theta \mu} -\mu \theta^2+\theta^2 Y_{t-2}+\theta \varepsilon_{t-1}+\varepsilon_t\\
	&amp;=\mu+\theta^2(Y_{t-2}-\mu)+\theta \varepsilon_{t-1}+\varepsilon_t
\end{align}$$`

---

`$$\begin{align}
	\text{Similarly, by substituting for}\, Y_{t-2}, \text{we get}\\
	&amp;=\mu+\theta^3(Y_{t-3}-\mu)+\varepsilon_t+\varepsilon_t+\theta \varepsilon_{t-1}+\theta^2 \varepsilon_{t-2}\\
	&amp;\vdots\\
    &amp;=\mu+\theta^n(Y_{t-n}-\mu)+\sum_{j=0}^{n-1}\theta^j \varepsilon_{t-j}\\
\end{align}$$`

When `\(n\longrightarrow \infty\)` and `\(\theta &lt;1\)` (remember the stationarity condition) , above equation boils down to 

`$$\begin{equation}
		Y_t=\mu+\sum_{j=0}^{n-1}\theta^j \varepsilon_{t-j}\\
		\label{eqtrar}
\end{equation}$$`

In the same manner, we can try to see whether an `\(\texttt{MA(1)}\)` process can be transformed into some kind of `\(\texttt{AR}\)` process. 

`$$\begin{align}
		MA(1) &amp; =\mu+\varepsilon_t+\varepsilon_{t-1}\\
		&amp; = \mu+Y_t-\delta - \theta Y_{t-1}+Y_{t-1}-\delta-\theta Y_{t-2} \\
		&amp;=\frac{\delta}{1-\theta}-2 \delta+(1-\theta)Y_{t-1}-\theta Y_{t-2} \\
		&amp;=\sigma_0+\sigma_1 Y_{t-1}+\sigma_2 Y_{t-2}
		\label{}
\end{align}$$`

---




class: inverse,middle,center 

# Formulating ARMA process

---

In this section, we start to define more general autoregressive and moving average process. In previous section, we have already observed a general framework of `\(MA(q)\)` model.

Obviously it is very much possible to combine the `\(MA(q)\)` and `\(AR(p)\)` and come up with a `\(ARMA(p,q)\)` specification of the following form:

`$$\begin{equation}
    y_t=\theta_1 y_{t-1}+\theta_2 y_{t-2}+\dotsb+\theta_p y_{t-p}+\varepsilon_t+\alpha \varepsilon_{t-1}+\dotsb+\alpha_q \varepsilon_{t-q}
\end{equation}$$`

???
Now, the next million dollar question is  when to choose an `\(AR\)`, `\(MA\)` or `\(ARMA\)`  Verbeek remains quite vague about this at this point. He says, `\(AR\)` and `\(MA\)` basically same time of series. It's just a matter of parsimony. What's the meaning of this? We have previously seen that `\(AR(1)\)` can be expressed as infinte lag order of `\(MA\)` series. So, what does parsimony means here? Does this mean that if we weant a smaller model then we choose `\(AR(1)\)` over infinite order `\(MA\)`? Verbeek says it will be clear later. At this point, it is difficult to know. We will postpone the discussion for now, come back to it later. Now we will talk briefly on `\(\textbf{Lag Operator}\)`.

This is just an example of `\(MA(q)\)`

`$$\begin{equation}
	y_t=\varepsilon_t+\alpha_1 \varepsilon_{t-1}+\alpha_2 \varepsilon_{t-2}+\dotsb+\alpha_q \varepsilon_{t-q}
	\label{eq:maq}
\end{equation}$$`

Here, `\(y_t=Y_t-\mu\)` and `\(\varepsilon_t\)` is a white noise process. It also means that the demeaned series `\(y_t\)` is a weighted combination (can we say weighted average also?) of `\(q+1\)` white noise terms. Here is `\(q+1\)` terms since the `\(q\)` starts from the second term. 
On the other hand, an autoregressive process of order `\(p\)`, which is denoted as `\(AR(p)\)`, can be expressed as:

`$$\begin{equation}
    y_t=\theta_1 y_{t-1}+\theta_2 y_{t-2}+\theta_3 y_{t-3}+\dotsb+\theta_p y_{t-p}+\varepsilon_t
    \label{eq:arq}
\end{equation}$$`

---

## Lag operator

In the notation of time series model, it is often convenient to use lag operator($L$) or backshift operator($B$) which some author use. But we will stick to `\(L\)` here. Let's see a use:

`$$\begin{equation}
    Ly_t=y_{t-1}\\
    L^2y_t=L.Ly_t=L.y_{t-1}=y_{t-2}\\
    \vdots \\
    L^q y_t=y_{t-q}
    \label{}
\end{equation}$$`

There are other relationships involving `\(L\)`, such as `\(L^0 \equiv 1\)` and `\(L^{-1}=y_{t+1}\)`. Use of this notions makes life much simpler in specifying a long time series specification such as an `\(ARMA\)` model quite concisely. For example, let's start with an `\(AR(1)\)` model:

`$$\begin{equation}
    \begin{split}
        y_t &amp; =\theta y_{t-1}+\varepsilon_t \\
            &amp; =\theta L.y_t+\varepsilon_t \\
            y_t-\theta Ly_t &amp; = \varepsilon_t \\
            (1-\theta L)y_t &amp;= \varepsilon_t
    \end{split}
    \label{eq:eqL}
\end{equation}$$`

---

We can refer to above equation as follows: here, `\(y_t\)` and it's one period lag `\(y_{t-1}\)` , on the right hand side, is a combination with weights of 1 and `\(-\theta(L)\)` and equals a white noise process ($\varepsilon_t$). In general, we can write `\(AR(p)\)` as follows:

`$$\begin{equation}
    \theta(L)y_t=\varepsilon_t
    \label{eq:eqargen}
\end{equation}$$`


Here `\(\theta(L)\)` is a polynomial `\(\footnote{What's the definition of polynomila?}\)` of order `\(p\)` .  `\(\theta(L)\)` is a function of `\(L\)`. We could write it like `\(f(L)\)`. Now, `\(\theta(L)\)` can be expressed as: 

`$$\begin{equation}
    \theta(L)=1-\theta L -\theta_2 L^2-\dotsb-\theta_p L^p
    \label{eqlagpol}
\end{equation}$$`

We can interpret lag polynomial as a filter. When we apply it to a time series, it produces a new series. 

---

## Characteristics of Lag Polynomial

Suppose we apply a lag polynomial to a time series. Then we apply another lag polynomial on the top of that series. This is equivalent to applying product of two lag polynomial on the original series. We can also define the inverse of a filter, which is the inverse of polynomial. The inverse of `\(\theta(L)\)` is `\(\theta^{-1}(L)\)` and we can write `\(\theta(L) \theta^{-1}(L)=1\)` . 


`$$\begin{equation}
    (1-\theta L)^{-1}=\sum_{j=0}^{\infty}\theta^j L^j
    \label{eq:lagpol2}
\end{equation}$$`

provided that `\(|\theta|&lt;1\)`. Now the question is how to prove that claim? 
Now let's start with simply `\(\sum_{j=0}^\infty \theta^j\)`. 

`$$\begin{align}
    \sum_{j=0}^\infty &amp; = \theta_0+\theta_1+\dotsb \\
    &amp; =\frac{1-\theta^\infty}{1-\theta} \quad \text{provided that} \quad |\theta|&lt;1 \\
    &amp;=\frac{1}{1-\theta}
    \label{eq:eqthet}
\end{align}$$`

???
Now the following statement I really don't understand. 

If `\(\theta(L)\)` is a finite order polynomial, then `\(\theta{-1}L\)` is a infinite order polynomial in `\(L\)`. 

---

Now let's see this version:
`$$\begin{align}
    \sum_{j=0}^\infty \theta^j L^j &amp; = \theta^0 L^0+ \theta^1 L^1 + \theta^2 L^2+ \dotsb \\
    &amp; = 1+\theta L + \theta^2 L^2+\dotsb \\
    &amp; = \frac{1-(\theta L)^\infty}{1-\theta L} \\
    &amp; = \frac{1-\theta^\infty L^\infty}{1-\theta L} \\
    &amp; = \frac{1}{1-\theta L} \quad \text{by} \quad |\theta|&lt;1\\
    &amp; = (1-\theta L)^{-1} \\
    \text{So we find that,}
    \sum_{j=0}^\infty \theta^j L^j &amp; = \frac{1}{1-\theta L}=(1-\theta L)^{-1}
    \label{eqthl}
\end{align}$$`

Now, we have seen in the above equation that `\((1-\theta L)y_t=\varepsilon_t\)`. 

---

From this it follows that:

`$$\begin{align}
    (1-\theta L)^{-1}(1-\theta L) y_t &amp; = (1-\theta L)^{-1} \varepsilon_t \\
    y_t &amp; = (1-\theta L)^{-1} \varepsilon_t \\
    \text{or,} \qquad y_t &amp; = \sum_{j=0}^\infty \theta^j L^j \varepsilon_t \\
    \text{From, the previous definition of Lag operator($L$)}
    L\varepsilon_t &amp; = \varepsilon_{t-1} \\
    L^2 \varepsilon_t &amp; =\varepsilon_{t-2} \\
    \vdots \\
    L^j \varepsilon_t &amp;= \varepsilon_{t-j} \\
    \text{Therefore, we can write}
    y_t &amp; = \sum_{j=0}^\infty \theta^j \varepsilon_{t-j}
    \label{}
\end{align}$$`

---

## From `\(MA(1)\)` to `\(AR(\infty)\)`

It corresponds to same derivation where we have shown that `\(AR(1)\)` corresponds to `\(MA(\infty)\)` series. We have said previously that `\(MA(1)\)` can also be transformed into a `\(AR\)` series but could not show it quite mathematically. Armed with lag operator `\((L)\)`, we can try to show it here. 

`$$\begin{align}
    \text{We know that in}\, MA(1) \\
    y_t   &amp;= \varepsilon_t+\alpha \varepsilon_{t-1} \\
          &amp;= \varepsilon_t+\alpha L \varepsilon_t \\
          &amp;= (1+\alpha L)\varepsilon_t
(1+\alpha L)^{-1}y_t &amp;= \varepsilon_t
\end{align}$$`

---

Now, let's see how we can define `\((1+\alpha L)^{-1}\)`.

`$$\begin{align}
    (1+\alpha L)^{-1} &amp;= \frac{1}{(1+\alpha L)} \\
    &amp; = \frac{1-(-\alpha L)^{\infty}}{1-(-\alpha L)} \\
    &amp; = -\alpha L+(-\alpha L)^2+ (-\alpha L)^3+\dotsb \\
    &amp; = \sum_{j=0}^\infty (-\alpha L)^j \\
    \text{Therfore, we write that } \\
    (1+\alpha L)^{-1} &amp; = \sum_{j=0}^\infty (-\alpha L)^j \\
    \text{Basically, we see that} \\
    \varepsilon_t &amp;=\sum_{j=0}^\infty (-\alpha)^j (L)^j y_t\\
    &amp;=\sum_{j=0}^\infty (-\alpha)^j y_{t-j}
\end{align}$$`

---

Now, let's figure out `\(\varepsilon_{t-1}\)`. This is important since we write  `\(MA(1) = \alpha \varepsilon_{t-1}+\varepsilon_t\)`. What we will do is put the value of `\(\varepsilon_{t-1}\)` and keep `\(\varepsilon_t\)` as it is

`$$\begin{align}
    y_{t-1} &amp; =\varepsilon_{t-1}+\alpha \varepsilon_{t-2} \\
    &amp; =\varepsilon_{t-1} + \alpha L \varepsilon_{t-1} \\
    &amp; =\varepsilon_{t-1}(1+\alpha L) \\
\text{or,}\qquad \varepsilon_{t-1} &amp; =(1+\alpha L)^{-1} y_{t-1} \\
&amp; = \sum_{j=0}^\infty (-\alpha L)^j y_{t-1} \\
&amp; = \sum_{j=0}^\infty (-\alpha)^j L^j y_{t-1} \\ 
&amp; = \sum_{j=0}^\infty (-\alpha)^j  y_{t-j-1} \\                    
\text{So, we can write}
\varepsilon_{t-1} &amp;= \sum_{j=0}^\infty (-\alpha)^j  y_{t-j-1} \\                    
\text{Now getting back to the}\, MA(1) \\
y_t &amp; =\alpha \varepsilon_{t-1}+\varepsilon_t \\
    &amp; =\alpha \sum_{j=0}^\infty (-\alpha)^j  y_{t-j-1}+ \varepsilon_t \\ 
\end{align}$$`

---

We here require that \textit{lag polynomial} in the `\(MA(1)\)` which is `\((1+\alpha L)\)` is invertible. It can be shown that it is invertible only if `\(|\alpha|&lt;1\)`. `\(AR\)` representation is very convenient when we think that current behavior is determined by past behavior. `\(MA\)` representation is quite useful for determining variance and covariance of the process. It is not quite clear how so but it might be clear in the subsequent analysis. 


---

## Parsimonious representation of ARMA

We have seen previously the following `\(ARMA\)` representation:

`$$\begin{gather}
    y_t  =\theta_1 y_{t-1}+\theta_2 y_{t-2}+\dotsb+\theta_p+\varepsilon_t+\alpha \varepsilon_{t-1}+\dotsb+\alpha_q \varepsilon_{t-q} \\
    \text{or,} \qquad y_t-\theta_1 y_{t-1}-\dotsb-\theta_p y_{t-p}  =(\alpha(L))\varepsilon \\
\text{or,} \qquad \theta (L) y_t  =\alpha (L) \varepsilon_t \\
\text{Now, if we think lag polynomical in}\, AR(1)\, \text{is invertible, then}\\ 
y_t  =\theta^{-1}(L) \alpha (L) \varepsilon_t \\
\text{On the other hand, if lag polynomial in}\,  MA(1)\, \text{is invertible, then we can write} \\
\alpha^{-1}(L) \theta (L) y_t  =\varepsilon_t
    \label{eq:eqpararma}
\end{gather}$$`

---




class: inverse,middle,center

# Invertibility of Lag Polynomial

---

## Second Order Polynomial

Let's consider this second order polynomial:

`$$\begin{equation}
        1-\theta_1 L-\theta_2 L^2
        \label{eq:eq2pn}
\end{equation}$$`

How to solve this type of question? Well this is typically a equation of the following form:

`$$\begin{equation} \label{eq2gn}
        ax^2+bx+c
\end{equation}$$`

Here `\(a=-\theta_2, b=-\theta_1, c=1\)`. Now the typical solution for this type of equation is:

`$$\begin{align}
        x &amp; =\frac{-b\pm \sqrt{b^2-4ac}}{2a}
        \label{eq:eqax2}
\end{align}$$`

???

We have seen that for the first order lag polynomical `\(1-\theta L\)` is invertible if `\(|\theta|&lt;1\)`. But it is not clear at this point what we mean by invertibility. Is this means whether we can find the value of `\(\frac{1}{1-\theta L}\)`? We can find the value of this expression even though `\(|\theta|&gt;1\)`, it's just negative\footnote{Need to have a clear understanding of invertibility}. Now, whatever invertibility is, let's move onand try to generalize this condition. First start with a second order polynomial. Then we will move onto higher degree of polynomial. 



---

Now let's think about a numerical example with the following example:  

`$$\begin{multline}\label{eqnumex}
        x^2-8x+15=x^2-3x-5x+15=x(x-3)-5(x-3) \\
             =(x-3)(x-5)=(3-x)(5-x)=15(1-x/3)(1-x/5)
\end{multline}$$`

Now let's put the above numerical example into a general mathematical form:

`$$\begin{equation}\label{eqphi}
       1-\theta_1 L-\theta_2 L^2=(1-\varphi_1 L)(1-\varphi_2 L)
\end{equation}$$`

---

Now look into \eqref{eqnumex} to relate itto the numerical example we have. If we think `\(\varphi_1=1/3\)` and `\(\varphi_2=1/5\)`, then `\(\varphi_1+\varphi_2=1/3+1/5=\frac{5+3}{15}=8/15\)`. Now we see what complicates the situation, its the `\(15\)`. If we compare \eqref{eqnumex} and \eqref{eqphi}, then we can say that `\(\theta_1=8\)`. Then `\(\varphi_1+\varphi_2=8/15\)` does not quite equal to `\(15\)`. But if we multiply it by `\(15\)` then `\(\frac{\cancel{15}}{8/\cancel{15}}=8\)`. 

The relationship is also such that `\(-\varphi_1 \varphi_2= \theta_2\)`. Now let's check whether this holds. We have `\(1/3 \cdot 1/5=1/15\)`. But again we have to multiply by `\(15\)` to get the desired result. Hence we get `\(1\)` which is equivalent to `\(\theta_2=1\)` which is the coefficient of `\(x^2\)` in \eqref{eqnumex}.Ok that is fine. 


???

Now if we look into \eqref{eqphi} and compare it with \eqref{eqnumex}, we find that `\(\varphi_1=1/3\)` and `\(\varphi_2=1/5\)`. But the presence of `\(15\)` complicates the matter. Anyway, let's whether we can resolve the issue or not. Author says that we can write `\(\varphi_1+\varphi_2=\theta_1\)`.It sounds like for the usual quadratic equation formula in \eqref{eq2gn} there is some relation. 



---

## Condition for invertibility

Let's have a look at the condition for invertibility in the polynomial:

`$$\begin{equation}
       |\varphi_1| &lt; 1 \qquad \text{and} |\varphi_2| &lt; 1
       \label{eqphi2}
\end{equation}$$`

 let's first specify the characteristic equation:

`$$\begin{equation}
       (1-\varphi_1 z)(1-\varphi_2 z)=0
       \label{eqphi3}
\end{equation}$$`

Solution of the equation in \eqref{eqphi3} can be expressed by two values of `\(z\)`: `\(z_1\)` and `\(z_2\)`. These are called \textbf{characteristic roots}.Why this is called characteristic equation or characteristic roots? Now the invertibility condition requires that `\(|\varphi_i|&lt;1\)` which translates into `\(|z_i&gt;1|\)`. If any solution that satisfies `\(|z_i|\ge 1\)` will result into a non-invertible polynomial. If `\(|z_i|\)` is exactly equal to `\(1\)`, then that solution is referred to as \textbf{unit root}. 

???

But the part that I don't understand is the condition for invertibility in the polynomial. It says that the polynomial of second order in \eqref{eqphi} will be invertible if both of the first order polynomials are also invertible. The first order polynomials are: `\((1-\varphi_1 L)\)` and `\((1-\varphi_2 L)\)` . These two have to be also invertible for the whole quadratic polynomial to be invertible \footnote{Have to look for details here.}. This implies that both `\(\varphi_1\)` and `\(\varphi_2\)` has to be less than one, that is :

Why this is so is not clear. Have to look into greater details. Anyways, author moves on to say that these requirements can also be formulated in terms of the so-called \textbf{characteristics equation}. Does it mean that charateristic equation is another way to mention this requirements of invertibility? Anyway,

When we look into this equation, the first thing that comes to mind is that why we are suddenly using `\(z\)` instead of `\(L\)` as we have done in \eqref{eqphi}.Well, my guess is that, it has been probably done to give a more generic look of the chracteristic equation. That's the only explanation I can find here.



---

**Invertibility with lag polynomial coefficients**

We can detect the presence of unit root by noting that the polynomial `\(\theta(z)\)` evaluated at `\(z=1\)` is zero if `\(\sum_{j=1}^{p} \theta_j=1\)`. Thus the presence of a first unit root can be verified by checking whether the sum of polynomial coefficients equals one. If the sum exceeds one, the polynomial is not invertible. 

Now have a look at the above statement and try to evaluate it with the following example. We are considering the following `\(AR(2)\)` model:

`$$\begin{equation}\label{eqexm}
    y_t=1.2 y_{t-1} - 0.32 y_{t-2}+\varepsilon_t 
\end{equation}$$`

???

Here the author discusses an easier to detect the presence of unit root in a lag polynonmial. Remember, here we are talking about lag polynomial not the equation itself. It may create some confusion in the beginning. It certainly did in my case. Now the statement that confused me is the following:

---

Now having a look at this equation \eqref{eqexm}, we might think that `\(\sum_{j=1}^{2}=1.2+(-0.32)=0.70\)` which is less than one. This might work just fine. We can express this equation a little bit more elaborately in the lag polynomial form: 

`$$\begin{gather}\label{eqexm2}
    y_t =1.2 L y_t-0.32 L^2 y_t+\varepsilon_t \\
    y_t(1-1.2L+0.32 L^2)=\varepsilon_t \\
    \text{We can also write this as} \\
    y_t(1-0.8L-0.4L+0.32L^2)=\varepsilon_t \\
    y_t[(1-0.8L)-0.4L(1-0.8L)]=\varepsilon_t \\
    y_t(1-0.8L)(1-0.4L)=\varepsilon_t 
    \text{This corresponds to the characteristics equation of the following form:} \\
    1-1.2z+0.32z^2=(1-0.4z)(1-0.8z)=0
\end{gather}$$`

---

Now let's revisit the following statement and try to relate it with the above statement:

We can detect the presence of unit root by noting that the polynomial `\(\theta(z)\)` evaluated at `\(z=1\)` is zero if `\(\sum_{j=1}^{p} \theta_j=1\)`. Thus the presence of a first unit root can be verified by checking whether the sum of polynomial coefficients equals one. If the sum exceeds one, the polynomial is not invertible. 

Now let's check what would be the value of characteristics equation when evaluated at `\(z=1\)`. 

`$$\begin{gather}
    1-1.2z+0.32z^2=1-1.2(1)+0.32(1)^2=1-1.2+0.32=1-0.70=0.30
\end{gather}$$`

---

We find the value of characteristics equation becomes `\(0.30\)` when z=1. When this value will become `\(0\)`? Now to see that let's consider the equation when this value might become zero. How about this equation:

`$$\begin{equation}\label{eqexmv2}
    y_t=1.2y_{t-1}-0.2y_{t-2}+\varepsilon_t
\end{equation}$$`

Here some of the coefficeints in the lag polynomial will be `\(1.2-0.2=1\)`.  Now the left hand side of the chracteristics equation with `\(z=1\)` will be:
\[1-1.2z+0.2z=1-1.2+0.2=0\]
Here we see that when sum of the coefficients of the polynomial is `\(1\)`, then value of the polynomial is zero when `\(z=1\)`. 

---

## Consequences of Invertibility 

`$$\begin{equation}\label{eqrw}
    y_t=1.2y_{t-1} +\varepsilon_t
\end{equation}$$`

Here, lag polynomial is much simpler: `\(1-1.2L\)`. Sum of the lag polynomial here is `\(1.2\)` where there is only first degree lag, so we have just one term here. It is greater than `\(1\)`, so it must be invertible. The real signficance of invertibility is that the series becomes non-stationary.  The border line case is \textbf{random walk} the value of lag coefficient is 1.

`$$y_t=y_{t-1}+\varepsilon_t$$`

The issue of whether lag polynomials are invertible or not is important for serveral reasons.For moving average models, it is important for estimation and predictions. For the autoregressive models, we already mentioned, it's ensures stationarity.

---

## Common Roots

Now here we will talk about common roots. Let's have a look into the `\(ARMA(2,1)\)` process of the following form: 
 
`$$\begin{gather} \label{}
	y_t=\theta_1 y_{t-1}+\theta_2 y_{t-2}+ \varepsilon_t + \alpha_1 \varepsilon_{t-1} \\
	y_t=\theta_1 L.y_t+\theta_2 L^2 y_t+\varepsilon_t+\alpha_1 L.\varepsilon_t \\
	y_t[1-\theta_1 L -\theta_2 L^2]=\varepsilon_t[1+\alpha_1 L]\\
	y_t(1-\varphi_1L)(1-\varphi_2 L)=\varepsilon_t(1+\alpha_1 L)\\
	\text{Now, if}\, \alpha_1=-\varphi_1 \\
	y_t(1+\alpha_1 L)(1-\varphi_2 L)=\varepsilon_t(1+\alpha_1 L)\\
	y_t \cancel{(1+\alpha_1 L)}(1-\varphi_2 L)=\varepsilon_t\cancel{(1+\alpha_1 L)}
	y_t(1-\varphi_2 L)=\varepsilon_t
\end{gather}$$`

In the above we see that we start with a process `\(ARMA(2,1)\)` and end up with a `\(ARMA(2-1,1-1)\)` or `\(AR(1)\)` process. Because of the cancelling root, it seem they are equivalent which is wrong. In general. becasue of one canceling root `\(ARMA(p,q)\)` can be written as `\(ARMA(p-1, q-1)\)`. 

An example can better illustrate the situation but above explantion might suffice here. The issue is that with a common cancelling root it is difficult to identify which one is the real underlying stochastic process. 

---

A time series will often have well-defined components: trend and seasonal patterns 

A well chosen linear regression may account for these non-stationary components

In this case the residuals from the fitted model should not contain noticeable trend or seasonal patterns 

But unfortunately that does not happen always 


Residuals will usually be correlated in time, this is not accounted for in the fitted regression model 


This happens because similar values may cluster together in time 

---

Adjacent observations may be negatively correlated, for example high monthly sales figure may be followed by an unusually low value because customers have supplies left over from the previous month 


In this topic, we consider stationary models that may be suitable for residual series that contain no obvious trends of seasonal cycles

The fitted stationary models may then be combined with the fitted regression model to improve forecasts

The autoregressive models that were introduced often provide satisfactory models for the residual time series. 








    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:10",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
