
class: inverse,middle,center

# Moving Average Models

---

## Introduction

Another very simple time series model is moving average of order 1 or MA(1). This process is given by:

$$\begin{equation}
    Y_t=\mu+\varepsilon_t+\alpha \varepsilon_{t-1}
\end{equation}$$

In above equation, $Y_t$ is sum of constant mean plus weighted average of current and past error. Usually weighted average is in the form of $\alpha \varepsilon+(1-\alpha) \varepsilon$ form. I have to look further into it. Basically the values of $Y_t$ are defined in terms of drawings from White Noise processes $\varepsilon_t$. 

---

## Mean of MA(1)

Mean of $MA(1)$ process is pretty simple: 

$$\begin{equation}
    E(Y_t)=\mu \quad \because E(\varepsilon_t)=E(\varepsilon_{t-1})=0
\end{equation}$$

## Variance of MA(1)

$$\begin{equation}
    \begin{split}
	Var(Y_t) & =E[Y_t-E(Y_t)]^2\\
	& =E(\cancel{\mu}+\varepsilon_t+\alpha \varepsilon_{t-1}-\cancel{\mu})^2\\
	& =E(\varepsilon_t+\alpha \varepsilon_{t-1})^2\\
	& =E(\varepsilon_t)^2+\alpha^2 E(\varepsilon_{t-1}^2)\\
	& =\sigma^2+\alpha^2 \sigma^2\\ 
	& =\sigma^2(1+\alpha^2)
  \end{split}
\end{equation}$$

---

## Covariance of MA process

$$\begin{equation}
    \begin{split}
	Cov(Y_t,Y_{t-1}) & = E[Y_t-E(Y_t)][Y_{t-1}-E(Y_{t-1})]\\
		     & = E(\varepsilon_t+\alpha \varepsilon_{t-1})(\varepsilon_{t-1}+\alpha \varepsilon_{t-2})\\
		     & =\alpha E(\varepsilon_{t-1}^2) \quad \because \quad Cov(\varepsilon_t,\varepsilon_{t-k})=0 \quad \forall \,t \quad \text{when} \quad k\neq 0\\ 
		     & = \alpha \sigma^2 \\
	    Cov(Y_t, Y_{t-2}) & = E[Y_t-E(Y_t)][Y_{t-2}-E(Y_{t-2})]\\
		     & = E(\varepsilon_t+\alpha \varepsilon_{t-1})(\varepsilon_{t-2}+\alpha \varepsilon_{t-3})\\
		     & = 0 \quad \because \text{all cross covariance of error terms are zero}\\ 
	Similarly Cov(Y_t,Y_{t-k}) & = 0 \quad \forall \quad k\ge 2
			\end{split}
			\label{eqmacov}
\end{equation}$$

The equation above implies that $AR(1)$ and $MA(1)$ has very different autocovariance structure. 

---

## MA(1) simulation

Let's simulate MA(1)

```{r}
set.seed(1)
x.ma <- w <- rnorm(100)
for (t in 2:100) x.ma[t]  <- w[t] + 0.7 * w[t-1] 
```


```{r, out.width="70%"}
plot(x.ma,type="l")            
```

---

.pull-left[

```{r}
acf(x.ma)
```
           
]
.pull-right[

```{r}
pacf(x.ma)

```
]

---

## Model fitted to simulated MA(1)

```{r}
x.ma.ts <- ts(x.ma, st=1980, frequency=12)
plot(x.ma.ts)
```

---

```{r}
library(forecast)
auto.arima(x.ma.ts)
```


---

class: inverse,middle,center 

# MA(q) process: Definition and properties 

---

## MA process

A moving average (MA) process of order $q$ is a linear combination of the current white noise term

The q most recent past white noise terms and is defined by
$$y_t=e_t+\beta_1e_{t-1}+\ldots+\beta_1e_{t-q}$$ 

Here ${e_t}$ is white noise with zero mean and variance $\sigma_e^2$. 

---


## MA equation with backshift operator

The above equation can be rewritten in terms of the backward shift operator `B`

$$y_t=(1+\beta_1B+\beta_2B^2+\ldots+\beta_qB^q)e_t=\phi_q(B)w_t$$ 

Here $\phi_q$ is a polynomial of order $q$. 

MA processes consist of a finite sum stationary white noise terms 

They are stationary and hence have a time-invariant mean and autocovariance 

---

## Mean and variance of MA(q) process

The mean and variance for ${x_t}$ are easy to derive 

The mean is just zero because it is a sum of terms that all have a mean of zero 

The variance is $\sigma{^2}_{e}(1+\beta_1^2+\ldots+\beta^2_q)$ 

Each of the white noise terms has the same variance and the terms are mutually independent. 

---

## ACF of MA(q) process

* As we have previously seen, `Autocorrelation` as a function of lag order $(k)$ is called $autocorrelation function$ or $ACF$ 
* ACF plays a major role in understanding a time series. 
* It tells us how deep is the memory of the underlying stochastic process. 
* If it has long memory, as in the case of $AR(1)$, we would see that value of $\rho_k$ does not diminish to zero with increasing value of $k$ as quickly as the $MA(1)$ does.
* We already have seen previously that covariance between two periods, in a $MA(1)$ process, reduces to zero if  lag is just more than one period. 
* Therefore by looking into the ACF we can have a fair idea about the underlying time series stochastic process. 


---

In the case of $MA(1)$

$$\begin{align}
	\gamma_1 & =Cov(Y_t,Y_{t-1})=\alpha \sigma^2,\quad 
	\text{and}\\
	\gamma_0 & =Var(Y_t)= \sigma^2 (1+\alpha^2) \\
	\text{Therefore, from the above two we can write}
	\rho_1 & =\frac{\gamma_1}{\gamma_0} \\
	&=\frac{\alpha \cancel{\sigma^2}}{\cancel{\sigma^2}(1+\alpha^2)} 
	&=\frac{\alpha}{1+\alpha^2}
	\label{}
\end{align}$$

And in the case of $k \ge 2 $, we have also seen that $\gamma_k=0 \quad \forall k$. Therefore,

$$\begin{equation}
	\rho_k=0 \quad \forall \quad k
	\label{}
\end{equation}$$


Implication of the above derivation is that, a shock in $MA(1)$ will only last two period, $Y_t$ and $Y_{t-1}$ while  a shock in $AR(1)$ will affect all future observations with a decreasing effect, since $|\theta|<1$.

---


## Comparing AR(1) and MA(1)

We can generalize $AR(1)$ and $MA(1)$ by adding additional lag terms. In general, there is little difference between these two models. We express$\mathbf{AR(1)}$  as$\mathbf{MA(1)}$ by repeated substitution.We can rewrite \texttt{AR(1)} as an infinite order of moving average. We can see this in the following:

$$\begin{align}
	Y_t &=\delta+\theta Y_{t-1}+\varepsilon \\
	&=\delta+\theta [\delta+\theta Y_{t-2}+\varepsilon_{t-1}]+\varepsilon_t\\
	&=\delta+\theta \delta+\theta^2 Y_{t-2}+\theta \varepsilon_{t-1}+\varepsilon_ti \label{del}\\
\text{We also have previously found that}\\
\mu &=\frac{\delta}{1-\theta}\\
\text{or,}\quad \delta &=\mu (1-\theta)\\
\text{Now putting this back into equation}\\ 
	&=\mu (1-\theta)+\theta \mu (1-\theta)+\theta^2 Y_{t-2}+\theta \varepsilon_{t-1}+\varepsilon_t\\
	&=\mu - \mu \theta+ \theta \mu -\mu \theta^2+\theta^2 Y_{t-2}+\theta \varepsilon_{t-1}+\varepsilon_t\\
	&=\mu - \cancel{\mu \theta}+ \cancel{\theta \mu} -\mu \theta^2+\theta^2 Y_{t-2}+\theta \varepsilon_{t-1}+\varepsilon_t\\
	&=\mu+\theta^2(Y_{t-2}-\mu)+\theta \varepsilon_{t-1}+\varepsilon_t
\end{align}$$

---

$$\begin{align}
	\text{Similarly, by substituting for}\, Y_{t-2}, \text{we get}\\
	&=\mu+\theta^3(Y_{t-3}-\mu)+\varepsilon_t+\varepsilon_t+\theta \varepsilon_{t-1}+\theta^2 \varepsilon_{t-2}\\
	&\vdots\\
    &=\mu+\theta^n(Y_{t-n}-\mu)+\sum_{j=0}^{n-1}\theta^j \varepsilon_{t-j}\\
\end{align}$$

When $n\longrightarrow \infty$ and $\theta <1$ (remember the stationarity condition) , above equation boils down to 

$$\begin{equation}
		Y_t=\mu+\sum_{j=0}^{n-1}\theta^j \varepsilon_{t-j}\\
		\label{eqtrar}
\end{equation}$$

In the same manner, we can try to see whether an $\texttt{MA(1)}$ process can be transformed into some kind of $\texttt{AR}$ process. 

$$\begin{align}
		MA(1) & =\mu+\varepsilon_t+\varepsilon_{t-1}\\
		& = \mu+Y_t-\delta - \theta Y_{t-1}+Y_{t-1}-\delta-\theta Y_{t-2} \\
		&=\frac{\delta}{1-\theta}-2 \delta+(1-\theta)Y_{t-1}-\theta Y_{t-2} \\
		&=\sigma_0+\sigma_1 Y_{t-1}+\sigma_2 Y_{t-2}
		\label{}
\end{align}$$

---


