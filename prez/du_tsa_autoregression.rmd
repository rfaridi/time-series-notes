class: inverse,center,middle

# Autogregressive models

---

## Specification

We will start with the simplest form of time-series model which is called first-order autoregressive models or AR(1).

--

$$\begin{equation}
    Y_t=\alpha+\theta Y_{t-1}+\varepsilon_t
\end{equation}$$

--

A simple way to model dependence between observations in different time periods would be that $Y_t$ depends linearly on the observation from the previous time period $Y_{t-1}$.

--

* Here $\varepsilon_t$ means serially uncorrelated innovation with mean of zero and a constant variance. 
--

* The process in the above equation is called a first order $\textbf{autoregressive process}$ or $AR(1)$ process. 

--

* This process tells us that the value of Y at time $t$ depends on a constant term plus $\theta$ times plus an unpredictable component $\varepsilon$. 

--

* Here we shall assume $|\theta|<1$.  

--

* The process underlying $\varepsilon_t$  called $\textbf{white noise process}$. 

--

* Here $\varepsilon$ will be always homoskedastic and will not show any autocorrelation. 

---

## Expected Value of AR(1)

The expected value of $Y_t$ can be solved from 

$$\begin{equation*} E(Y_t)=\delta+\theta E(Y_{t-1}) \end{equation*}$$

--

which, assuming that $E(Y_t)$ does not depend upon $t$, allows us to write 

$$E(Y_t)=E(Y_{t-1})=\mu \label{}$$

--

Remember, this is only true if $Y_t$ is stationary, that means it's statistical property does not depend on a particular time period, in other words, the mean is constant. We will also write $y_t=Y_t-\mu$. Now it follows that 

--

$$\begin{gather*}
    y_t+\mu=\delta+\theta (y_{t-1}+\mu)+\varepsilon \\ 
    \text{In the above we have seen that} \\ 
    \mu\equiv E(Y_t)=\frac{\delta}{(1-\theta)} \\ 
    \text{which means} \\ 
    \delta=\mu (1-\theta) \\ 
    \text{Now putting this value in the above equation} \\ 
    y_t+\mu=\mu (1-\theta)+\theta y_{t-1}+\theta \mu+\varepsilon \\ y_t+\cancel{\mu}=\cancel{\mu}-\mu \theta+\theta y_{t-1}+\theta \mu+\varepsilon \\ 
    y_t=\cancel{\mu \theta}+\theta y_{t-1}+\cancel{\theta \mu}+\varepsilon \\ 
    y_t=\theta y_{t-1}+\varepsilon 
\end{gather*}$$

???

Defining time series models in terms of $y_t$ rather than $Y_t$ is often notationally convenient. So $y_t$ has been introduced so that depiction of time models remain concise

---

Taking expectation, we get :
$$E(y_t)=\theta E(y_{t-1})+ E(\varepsilon)$$ 

--

Since $E(\varepsilon)=0$ we can write 
$$E(y_t)=\theta E(y_{t-1})$$ 

--

Remember, previously we said that $Y_t$ is stationary and as a result $E(Y_t)=E(Y_{t-1})$

--

Then we can write,

--

$$\begin{gather*}
    E(y_t+\mu) =E(y_{t-1}+\mu) \\
    E(y_t)+E(\mu)=E(y_{t-1})+E(\mu) \\
 \text{since}\,\mu\, \text{is a constant}\,E(\mu)=\mu  \\
    E(y_t)+\cancel{\mu}=E(y_{t-1})+\cancel{\mu} \\
    E(y_t)=E(y_{t-1}) \\
    \text{Now from the above, we can write} \\
    E(y_t)-\theta E(y_t)=0 \quad \text{since} \quad E(y_t)=E(y_{t-1}) \\
    E(y_t)=0
\end{gather*}$$

Exaplanation: 

$$ E(y_t) (1-\delta) = 0 $$  

If   $\delta\neq 1  $ then $1-\delta\neq 0$  
If $1-\delta\neq 0$   then $E(y_t) =0 $  


---

* Above results show that $y_t$ has a zero mean. But to have a non-zero mean we can add an intercept. 

--

* We also here note that $V(Y_t)=V(y_t)$. 

--

* The process described in above equation imposes certain restrictions on the stochastic process that generates $Y_t$. 

--

* Usually when we have a group of variables, we usually describe their joint distribution by  covariances. 

--

* Since, here are lagged version of other variables, we call it $\texttt{autocovariances}$ .  

--

* This is the so called $\texttt{stationarity}$ condition. 

--

* Basically we require stationarity to derive dynamic properties of $Y_t$.

--

* Without deriving dynamic properties of $Y_t$, we can not forecast its values. 

---

## Variance of AR(1)

Previously we derived the constant mean of the distribution by imposing that mean does not depend on $t$. Now we will do the same thing with variances. 

--

First let's derive the variance:


--

$$\begin{equation*}
	\begin{split}
		Var(Y_t)&=Var(\delta+\theta Y_{t-1}+\varepsilon_t)\\
		 &=Var(\delta)+Var(\theta Y_{t-1}+\varepsilon_t) \\
		&=Var(\theta Y_{t-1}+\varepsilon_t) \quad \because \quad Var(\delta)=0\\
		&=\theta^2 Var(Y_{t-1})+Var(\varepsilon_t) 
	\end{split}
\end{equation*}$$

--

Therefore, we can write

    
Here we assume that $Cov(Y_{t-1} \varepsilon_t)=0$. Now, this is not too unrealistic in the sense that error at current period might not be correlated with the endogenous variable in the past.Now it's time to impose one of the stationarity condition, namely, variance of the time series process does not depend on time:

$$\begin{equation}\label{eqst}
		Var(Y_t)=Var(Y_{t-1})
	\end{equation}$$

---

Now using above equations,

$$\begin{gather}
    Var(Y_t) =\theta^2 Var(Y_t)+ \sigma^2 \quad \text{by} \quad Var(\varepsilon_t)=\sigma^2\\
		\text{or,} \quad Var(Y_t)(1-\theta^2) =\sigma^2\\
		\text{or,} \quad Var(Y_t) =\frac{\sigma}{1-\theta^2} \label{eqvar2}
\end{gather}$$


We see from the above equation  that $Var(Y_t)$ is indeed constant. We also see from above that we can only impose $Var(Y_t)=Var(Y_{t+1}$ if $|\theta|<1$ . This is the assumption we madel previously.This is actually the essence of $\texttt{Dickey-Fuller}$ test which tests whether this coefficient is less than one or not. 

---

## Covariances of AR(1)	

Now let's find the $Covariance$ between $Y_t$ and $Y_{t-1}$

$$\begin{equation} 
    \begin{split} 
	Cov(Y_t,Y_{t-1}) & =E(Y_t-E(Y_t))E(Y_{t-1}-E(Y_{t-1})) \\ 
		     & =E((Y_t-\mu)(Y_{t-1}-\mu))  \quad \because \quad E(Y_t)=E(Y_{t-1})=\mu \\
		     & =E(y_t y_{t-1}) \quad \text{by the definition of}\quad y_t\\
		     & =E((\theta y_{t-1}+\varepsilon_t)(y_{t-1})) \because \, y_t=\theta y_{t-1}+\varepsilon_t \\
                     & =\theta (E(y_{t-1})^2+E(\varepsilon_t y_{t-1}) \\
		     & =\theta \, E(Y_{t-1}-\mu)^2+E(\varepsilon_t y_{t-1})\\
		     & =\theta \, Var(Y_t)+E((\varepsilon_t-E(\varepsilon_t)) (Y_{t-1}-\mu))\\
		     & =\theta \, Var(Y_t)+Cov(\varepsilon_t,Y_t)\\
		     & =\theta \, Var(Y_t)\quad \because \quad Cov(\varepsilon_t,Y_{t-1})=0
		\label{eqcov}
    \end{split} 
\end{equation}$$

So, we have established that 

$$\begin{equation}
	Cov(Y_t,Y_{t-1})=\theta \frac{\sigma^2}{1-\theta^2}
	\label{eqcov2}
\end{equation}$$

---

Now let's take some higher order lags. Let's see what would be the covariance between $Y_t$ and $Y_{t-2}$, that is autocovariance of lag order 2.


$$\begin{equation}
		\begin{split}
		Cov(Y_t,Y_{t-2}) & =E(Y_t-E(Y_t))E(Y_{t-2}-E(Y_{t-2}))\\
		& =E((Y_t-\mu)(Y_{t-2}-\mu))\quad \because \quad E(Y_t)=E(Y_{t-2})=\mu\\
		& =E(y_t\, y_{t-2})\quad \text{by the definition of} Y_t\\
\text{Before continuing we have to figure out} \, y_t\\
		y_t & =\theta y_{t-1}+\varepsilon_t\\
		& =\theta (\theta y_{t-2}+\varepsilon_{t-1})+\varepsilon_t\\
		& =\theta^2 y_{t-2}+\theta \varepsilon_{t-1}+\varepsilon_t\\
\text{Now,putting this value back in equation}
		& =E((\theta^2 y_{t-2}+\theta \varepsilon_{t-1}+\varepsilon_t)y_{t-2})\\
		& =\theta^2 (E(y_{t-2})^2+E(\varepsilon_{t-1} y_{t-2})+E(\varepsilon_t y_{t-2}))\\
		& =\theta^2 \, E(Y_{t-2}-\mu)^2+Cov(\varepsilon_{t-1},y_{t-2})+Cov(\varepsilon_t,y_{t-2})\\
		& =\theta^2 \, Var(Y_t)\quad \because \quad Cov(\varepsilon_t,Y_{t-k})=0\\
		& =\theta^2 \, \frac{\sigma^2}{1-\theta^2}
		\end{split}
\end{equation}$$

---

$$\begin{equaiton}
   \begin{split} 
    \text{Therefore we see that,}
    & Cov(Y_t,Y_{t-1}) =\theta \frac{\sigma^2}{1-\theta^2}\\
    & Cov(Y_t,Y_{t-2}) =\theta^2 \frac{\sigma^2}{1-\theta^2}\\
    \vdots \\
    & Cov(Y_t,Y_{t-k}) =\theta^k \frac{\sigma^2}{1-\theta^2}
		\end{split}
\end{equation}$$

We have the following observation from the above Covariance formula


* As long as $\theta$ is non-zero, any two observation on $Y$ has non-zero correlation. This might imply that as long as $Y$ in period $t$ is correlated with it's immediate past value, that correlation carries through higher degree of past value.
* Since the value of $|\theta|<1$, this correlation diminishes in higher degree of lag-order. This is quite intuitive as itis natural to have weakerza correlation among values whic are far apart. 
* Covariance between any two time periods does not depend on any of the time periods.Rather it depends on how far they are apart, which is determined by $k$. This is one of the primary features of stationary time series. 	

---

